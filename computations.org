* Obtaining the dataset
  Dataset is available at [[https://data.caida.org/][CAIDA website]], but a permit is needed, as stated in their FAQ.
  More details about the various files provided can be found in their [[https://data.caida.org/datasets/passive-2018/README-2018.txt][README]].

  #+NAME: dataset_urls
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-125910.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130000.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130100.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130200.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130300.UTC.anon.pcap.gz

  This snippet will download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for ~xargs~ parameters details.
  ~wget -c~ ensure each file is downloaded once, even if it is stopped and resumed.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh :results none
    # read username and password from file
    read caida_user caida_password < secrets.txt

    for dataset_url in ${dataset_urls[@]}
    do
        # download dump
        wget -q -P data/pcap/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             $dataset_url

        # download nanosecond precision times
        wget -q -P data/times/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             "${dataset_url%%.pcap*}".times.gz
    done
  #+END_SRC

  Here follows a list of all downloaded files, automatically generated from provided links.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :results raw replace
    echo "#+NAME: records"
    for dataset_url in ${dataset_urls[@]}
    do
        pcap_file="- $(basename $dataset_url)"
        echo ${pcap_file%%.pcap*}
    done
  #+END_SRC

  #+RESULTS:
  #+NAME: records
  - equinix-nyc.dirA.20180315-125910.UTC.anon

* Preprocessing
  In this step we convert ~tcpdump~ files to ~csv~.

  We are interested only in some properties of the traffic, whose Wireshark keywords are further listed.
  Note that packet length is expressed in bytes and time is ignored, as it is taken by proper times file (more precise).

  #+BEGIN_SRC bash :var records=records :results output :tangle scripts/2_convert.sh
    # put "-e " before each field
    for record in "${records[@]}"
    do
        pcap_file="data/pcap/${record}.pcap.gz"
        times_file="data/times/${record}.times.gz"

        output_path="data/csv/${record}.csv.gz"

        # stop iteration immediately if output file already exists
        if [ -f $output_path ]; then
            echo "CONVERT: Jumping $record, output already exists"
            continue
        else
            echo "CONVERT: Working on $record"
        fi

        # custom awk script from tcpdump instead of crappy memory-leanking tshark
        # NOTE that all packets are IP, layer 2 header is removed by CAIDA project
        awk_snippet='BEGIN { IFS=" "; print "ip.src,port.src,ip.dst,port.dst,length,protocol" }
                           {
                             split($17, src, "\\.");
                             split($19, dst, "\\.");

                             # "." source ip/port couple
                             printf src[1] "." src[2] "." src[3] "." src[4] ",";
                             printf src[5] ",";

                             # destination ip/port couple
                             printf dst[1] "." dst[2] "." dst[3] "." dst[4] ",";
                             printf dst[5] ",";

                             print $16 "," $13
                           }'

        # read pcap file with tcpdump
        # remove added newline between packet infos
        # remove parenthesis
        # extract wanted fields
        # merge with timestamps, from dedicated file
        paste -d"," \
              <(echo "time," && zcat $times_file) \
              <(pv $pcap_file \
                    | gunzip -c \
                    | tcpdump -r - -ntv \
                    | sed ':begin;$!N;/)\n/s/\n//;tbegin;P;D' \
                    | sed 's/[():]//g' \
                    | awk "$awk_snippet") | gzip > $output_path
    done
  #+END_SRC

  #+RESULTS:
  : CONVERT: Working on dummy_capture

* Analysis
** Interarrival distribution
   Time deltas are stored in a proper ~csv~ in order to measure interarrival distribution.

   #+BEGIN_SRC bash :tangle scripts/4_interarrival.sh :var records=records :results none
     for record in "${records[@]}"
     do
         csv_file="data/csv/${record}.csv.gz"

         if [ ! -f $csv_file ]; then
             echo "INTERARRIVAL: no csv file available"
             continue
         else
             echo "INTERARRIVAL: Working on $csv_file"
         fi

         awk_snippet='BEGIN   { FS=","; OFS="," }
                      NR == 1 {
                         print "interarrival_times";
                      }
                      NR == 2 { prev_time = $1 }
                      NR > 2  {
                        delta = $1 - prev_time;
                        prev_time = $1;

                        # choose only half of the times (for memory constraints)
                        if (rand() <= .1) printf("%.10f\n", delta);
                      }'

         pv $csv_file \
             | gunzip -c \
             | awk "$awk_snippet" \
             | gzip -c > ${csv_file%%.*}_interarrival.csv.gz
     done
   #+END_SRC

   Finally plot the results.
   Check [[https://cran.r-project.org/web/packages/extrafont/README.html][here]] in order to use Charis SIL font.

   #+NAME: utils
   #+BEGIN_SRC R
     library(ggplot2)
     library(readr)
     library(extrafont)

     my_theme <- theme_bw() +
         theme(
             text = element_text(family = 'Charis SIL'),
             plot.title = element_text(hjust = 0.5)
         )
   #+END_SRC

   #+BEGIN_SRC R :var records=records :noweb yes :tangle scripts/5_interarrival_plot.r :results none
     <<utils>>

     for (record in records[[1]]) {
         gc()

         dataset <- as.data.frame(
             read_csv(paste('data/csv/', record, '_interarrival.csv.gz', sep = ''))
         )

         p <- ggplot(dataset, aes(x = interarrival_times * 1000)) +
             stat_ecdf(geom = "step") +
             labs(title = paste(name, 'dataset'),
                  x = 'Packet delay [ms]',
                  y = 'Probability density') +
             scale_x_log10() +
             my_theme

         ggsave(plot = p,
                filename = paste('plots/', record, '.pdf', sep=''),
                device = 'pdf',
                width = 3,
                height = 3,
                unit = 'in')
     }
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
