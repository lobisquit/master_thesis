* Obtaining the dataset
  Dataset is available at [[https://data.caida.org/][CAIDA website]], but a permit is needed, as stated in their FAQ.
  More details about the various files provided can be found in their [[https://data.caida.org/datasets/passive-2018/README-2018.txt][README]].

  #+NAME: dataset_urls
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-125910.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130000.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130100.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130200.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130300.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133200.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133300.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133400.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133500.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133600.UTC.anon.pcap.gz

  This snippet will download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for ~xargs~ parameters details.
  ~wget -c~ ensure each file is downloaded once, even if it is stopped and resumed.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh :results none
    # read username and password from file
    read caida_user caida_password < secrets.txt

    for dataset_url in ${dataset_urls[@]}
    do
        # download dump
        wget -P data/pcap/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             $dataset_url

        # download nanosecond precision times
        wget -P data/times/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             "${dataset_url%%.pcap*}".times.gz
    done
  #+END_SRC

  Here follows a list of all downloaded files, automatically generated from provided links.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :results raw replace
    echo "#+NAME: records"
    for dataset_url in ${dataset_urls[@]}
    do
        pcap_file="- $(basename $dataset_url)"
        echo ${pcap_file%%.pcap*}
    done
  #+END_SRC

  #+RESULTS:
  #+NAME: records
  - equinix-nyc.dirA.20180315-125910.UTC.anon
  - equinix-nyc.dirA.20180315-130000.UTC.anon
  - equinix-nyc.dirA.20180315-130100.UTC.anon
  - equinix-nyc.dirA.20180315-130200.UTC.anon
  - equinix-nyc.dirA.20180315-130300.UTC.anon
  - equinix-nyc.dirA.20180816-133200.UTC.anon
  - equinix-nyc.dirA.20180816-133300.UTC.anon
  - equinix-nyc.dirA.20180816-133400.UTC.anon
  - equinix-nyc.dirA.20180816-133500.UTC.anon
  - equinix-nyc.dirA.20180816-133600.UTC.anon

* Preprocessing
  In this step we convert ~tcpdump~ files to ~csv~.

  We are interested only in some properties of the traffic, which we extract with a custom ~awk~ script from ~tcpdump~ formatted output.
  Note that packet length is expressed in bytes and time is ignored, as it is taken by proper times file (more precise).

  #+BEGIN_SRC bash :var records=records :results output :tangle scripts/2_convert.sh
    for record in "${records[@]}"
    do
        pcap_file="data/pcap/${record}.pcap.gz"
        times_file="data/times/${record}.times.gz"

        output_path="data/csv/${record}.csv.gz"

        # stop iteration immediately if output file already exists
        if [ -f $output_path ]; then
            echo "CONVERT: Jumping $record, output already exists"
            continue
        else
            echo "CONVERT: Working on $record"
        fi

        # custom awk script from tcpdump instead of crappy memory-leanking tshark
        # NOTE that all packets are IP, layer 2 header is removed by CAIDA project
        awk_snippet='BEGIN { IFS=" "; print "ip.src,port.src,ip.dst,port.dst,length,protocol" }
                           {
                             split($17, src, "\\.");
                             split($19, dst, "\\.");

                             # "." source ip/port couple
                             printf src[1] "." src[2] "." src[3] "." src[4] ",";
                             printf src[5] ",";

                             # destination ip/port couple
                             printf dst[1] "." dst[2] "." dst[3] "." dst[4] ",";
                             printf dst[5] ",";

                             print $16 "," $13
                           }'

        # read pcap file with tcpdump
        # remove added newline between packet infos
        # remove parenthesis
        # extract wanted fields
        # merge with timestamps column from dedicated file
        paste -d"," \
              <(echo "time" && zcat $times_file) \
              <(pv $pcap_file \
                    | gunzip -c \
                    | tcpdump -r - -ntv \
                    | sed ':begin;$!N;/)\n/s/\n//;tbegin;P;D' \
                    | sed 's/[():]//g' \
                    | awk "$awk_snippet") | gzip > $output_path
    done
  #+END_SRC

  #+RESULTS:
  : CONVERT: Working on dummy_capture

  Just to be sure, ~csv~ files can be validated with ~xsv~ tool.
  #+BEGIN_SRC bash
    for record in "${records[@]}"
    do
        pv "data/csv/${record}.csv.gz" | gunzip -c | xsv stats
    done
  #+END_SRC

* Analysis
** Interarrival distribution
   Time deltas are stored in a proper ~csv~ in order to measure interarrival distribution.

   #+BEGIN_SRC bash :tangle scripts/3_interarrival.sh :var records=records :results none
     for record in "${records[@]}"
     do
         csv_file="data/csv/${record}.csv.gz"

         if [ ! -f $csv_file ]; then
             echo "INTERARRIVAL: no csv file available"
             continue
         else
             echo "INTERARRIVAL: Working on $csv_file"
         fi

         awk_snippet='BEGIN   { FS=","; OFS="," }
                      NR == 1 {
                         print "interarrival_times";
                      }
                      NR == 2 { prev_time = $1 }
                      NR > 2  {
                        delta = $1 - prev_time;
                        prev_time = $1;

                        # choose only part of the measures (for memory constraints)
                        if (rand() <= .1) printf("%.10f\n", delta);
                      }'

         pv $csv_file \
             | gunzip -c \
             | awk "$awk_snippet" \
             | gzip -c > "data/csv/${record}_interarrival.csv.gz"
     done
   #+END_SRC

   Finally plot the results.
   Check [[https://cran.r-project.org/web/packages/extrafont/README.html][here]] in order to use Charis SIL font.

   #+NAME: utils
   #+BEGIN_SRC R
     library(ggplot2)
     library(readr)
     library(extrafont)

     my_theme <- theme_bw() +
         theme(
             text = element_text(family = 'Charis SIL'),
             plot.title = element_text(hjust = 0.5)
         )
   #+END_SRC

   #+BEGIN_SRC R :var records=records :noweb yes :tangle scripts/4_interarrival_plot.r :results none
     <<utils>>

     for (record in records[[1]]) {
         gc()

         dataset <- as.data.frame(
             read_csv(paste('data/csv/', record, '_interarrival.csv.gz', sep = ''))
         )

         p <- ggplot(dataset, aes(x = interarrival_times * 1000)) +
             stat_ecdf(geom = "step") +
             labs(title = paste(record, 'dataset'),
                  x = 'Packet delay [ms]',
                  y = 'Probability density') +
             scale_x_log10() +
             my_theme

         ggsave(plot = p,
                filename = paste('plots/', record, '_interarrival_CDF.pdf', sep=''),
                device = 'pdf',
                width = 3,
                height = 3,
                unit = 'in')
     }

     ## remove annoying Rplots.pdf file
     system("rm -f Rplots.pdf")
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # org-export-babel-evaluate: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
