* Obtaining the dataset
  Dataset is available at [[https://data.caida.org/][CAIDA website]], but a permit is needed, as stated in their FAQ.
  More details about the various files provided can be found in their [[https://data.caida.org/datasets/passive-2018/README-2018.txt][README]].

  #+NAME: dataset_urls
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-125910.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130000.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130100.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130200.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180315-130000.UTC/equinix-nyc.dirA.20180315-130300.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133200.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133300.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133400.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133500.UTC.anon.pcap.gz
  - https://data.caida.org/datasets/passive-2018/equinix-nyc/20180816-130000.UTC/equinix-nyc.dirA.20180816-133600.UTC.anon.pcap.gz

  This snippet will download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for ~xargs~ parameters details.
  ~wget -c~ ensure each file is downloaded once, even if it is stopped and resumed.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh :results none
    # read username and password from file
    read caida_user caida_password < secrets.txt

    for dataset_url in $(echo $dataset_urls | sed 's/ /\n/g')
    do
        # download dump
        wget -P data/pcap/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             $dataset_url

        # download nanosecond precision times
        wget -P data/times/ -c \
             --http-user=$caida_user \
             --http-password=$caida_password \
             "${dataset_url%%.pcap*}".times.gz
    done
  #+END_SRC

  Here follows a list of all downloaded files, automatically generated from provided links.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :results raw replace
    echo "#+NAME: records"
    for dataset_url in ${dataset_urls[@]}
    do
        pcap_file="- $(basename $dataset_url)"
        echo ${pcap_file%%.pcap*}
    done
  #+END_SRC

  #+RESULTS:
  #+NAME: records
  - equinix-nyc.dirA.20180315-125910.UTC.anon
  - equinix-nyc.dirA.20180315-130000.UTC.anon
  - equinix-nyc.dirA.20180315-130100.UTC.anon
  - equinix-nyc.dirA.20180315-130200.UTC.anon
  - equinix-nyc.dirA.20180315-130300.UTC.anon
  - equinix-nyc.dirA.20180816-133200.UTC.anon
  - equinix-nyc.dirA.20180816-133300.UTC.anon
  - equinix-nyc.dirA.20180816-133400.UTC.anon
  - equinix-nyc.dirA.20180816-133500.UTC.anon
  - equinix-nyc.dirA.20180816-133600.UTC.anon

* Preprocessing
  In this step we convert ~tcpdump~ files to ~csv~.

  We are interested only in some properties of the traffic, which we extract with a custom ~awk~ script from ~tcpdump~ formatted output.
  This is a workaround for a nasty memory leak problem on ~tshark~ side.

  Note that
  - all packets are viewed from layer 3 (IP and above information)
  - packet length is expressed in bytes
  - time is expressed in nanoseconds and it is taken by proper times file

  #+NAME: awk_convert
  #+BEGIN_SRC awk
    BEGIN { IFS=" "; print "ip.src,port.src,ip.dst,port.dst,length,protocol" }
    {
        split($17, src, "\\.");
        split($19, dst, "\\.");

        # "." source ip/port couple
        printf src[1] "." src[2] "." src[3] "." src[4] ",";
        printf src[5] ",";

        # destination ip/port couple
        printf dst[1] "." dst[2] "." dst[3] "." dst[4] ",";
        printf dst[5] ",";

        print $16 "," $13
    }
  #+END_SRC

  #+BEGIN_SRC bash :var records=records :results none :tangle scripts/2_convert.sh :noweb yes
    for record in $(echo $records | sed 's/ /\n/g')
    do
        pcap_file="data/pcap/${record}.pcap.gz"
        times_file="data/times/${record}.times.gz"

        output_path="data/csv/${record}.csv.gz"

        # stop iteration immediately if output file already exists
        if [ -f $output_path ]; then
            echo "CONVERT: Skipping $record, output already exists"
            continue
        else
            echo "CONVERT: Working on $record"
        fi

        # read pcap file with tcpdump
        # remove added newline between packet infos
        # remove parenthesis
        # extract wanted fields
        # merge with timestamps column from dedicated file
        # those timestamps, however, need to be premultiplied (using bc) since awk loses precision
        # when dealing with floating point numbers in order to remove the fractional part
        paste -d"," \
              <(echo "time" && zcat $times_file) \
              <(pv $pcap_file \
                    | gunzip -c \
                    | tcpdump -r - -ntv \
                    | sed ':begin;$!N;/)\n/s/\n//;tbegin;P;D' \
                    | sed 's/[():]//g' \
                    | awk ' \
                      <<awk_convert>>') \
            | gzip > $output_path
    done
  #+END_SRC

  Just to be sure, ~csv~ files can be validated with ~xsv~ tool.
  #+BEGIN_SRC bash
    for record in $(echo $records | sed 's/ /\n/g')
    do
        pv "data/csv/${record}.csv.gz" | gunzip -c | xsv stats
    done
  #+END_SRC

* Analysis
** Interarrival distribution
   Time deltas are stored in a proper ~csv~ in order to measure interarrival distribution.

   #+NAME: awk_interarrival
   #+BEGIN_SRC awk
     BEGIN   { FS="," }
     NR == 2 { prev_time = $1 }
     NR > 2  {
         # choose only part of the measures (for memory constraints)
         if (rand() <= .1) print $1 "-" prev_time;

         prev_time = $1;
     }
   #+END_SRC

   #+BEGIN_SRC bash :tangle scripts/3_interarrival.sh :var records=records :results none :noweb yes
     for record in $(echo $records | sed 's/ /\n/g')
     do
         csv_file="data/csv/${record}.csv.gz"

         if [ ! -f $csv_file ]; then
             echo "INTERARRIVAL: no csv file available"
             continue
         else
             echo "INTERARRIVAL: Working on $csv_file"
         fi

         (
             echo "deltas" \
                 && pv $csv_file \
                     | gunzip -c \
                     | awk ' \
                       <<awk_interarrival>>' \
                     | bc -l
         ) \
             | gzip -c > "data/csv/${record}_interarrival.csv.gz"
     done
   #+END_SRC

   Finally plot the results.
   Check [[https://cran.r-project.org/web/packages/extrafont/README.html][here]] in order to use Charis SIL font.

   #+NAME: utils
   #+BEGIN_SRC R
     library(ggplot2)
     library(readr)
     library(extrafont)
     library(MASS)

     my_theme <- theme_bw() +
         theme(
             text = element_text(family = 'Charis SIL'),
             plot.title = element_text(hjust = 0.5)
         )
   #+END_SRC

   #+BEGIN_SRC R :var records=records :noweb yes :tangle scripts/4_interarrival_plot.r :results none
     <<utils>>

     for (record in records[[1]]) {
         gc()

         dataset <- as.data.frame(
             read_csv(paste('data/csv/', record, '_interarrival.csv.gz', sep = ''))
         )

         p <- ggplot(dataset, aes(x = deltas)) +
             stat_ecdf(geom = "step") +
             labs(title = paste(record, 'dataset'),
                  x = 'Packet delay [ms]',
                  y = 'Probability density') +
             scale_x_log10() +
             my_theme

         ggsave(plot = p,
                filename = paste('plots/', record, '_interarrival_CDF.pdf', sep=''),
                device = 'pdf',
                width = 3,
                height = 3,
                unit = 'in')
     }

     ## remove annoying Rplots.pdf file
     system("rm -f Rplots.pdf")
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # org-export-babel-evaluate: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
