#+PROPERTY: header-args :mkdirp yes
#+PROPERTY: header-args:python :shebang "#!/usr/bin/python3 \n# -*- coding: utf-8 -*-\nfrom __future__ import print_function"
#+STARTUP: indent

* Retrieve datasets
Download all needed files from Aachen city repository.

These are under a special license: "Data license Germany - Attribution - Version 2.0".
#+NAME: population_density_link
http://offenedaten.aachen.de/dataset/81650028-ef21-4f1b-a991-9e3a3f01c729/resource/460bfe18-7df4-49fb-b5d0-6dfc1d0cffd5/download/20170630opendataaachen-daten-statistische-bezirkealle.csv

#+NAME: district_map_link
http://offenedaten.aachen.de/dataset/5ea893af-8f1d-4658-9066-8f05daed1022/resource/6dfc1b81-26d9-4ed8-b8c4-a61013659f51/download/statistischebezirkeaachen.zip

#+NAME: NRW_map_link
http://download.geofabrik.de/europe/germany/nordrhein-westfalen-latest-free.shp.zip

For the roads details, see [[https://simonb83.github.io/making-a-map-in-matplotlib.html][here]].

#+BEGIN_SRC bash :var population_density=population_density_link district_map=district_map_link NRW_map=NRW_map_link :results none :tangle scripts/aachen_net/01_download.sh
  mkdir -p data/aachen_net/

  # download
  wget -c $population_density -O data/aachen_net/20170630_population_density_temp.csv
  wget -c $district_map -O data/aachen_net/district_map.zip
  wget -c $NRW_map -O data/aachen_net/NRW_map.zip

  # preprocess
  awk -F, '{print $1 "," $3}' data/aachen_net/20170630_population_density_temp.csv > data/aachen_net/20170630_population_density.csv
  rm -f data/aachen_net/20170630_population_density_temp.csv

  # gather city district borders
  unzip -p data/aachen_net/district_map.zip StatistischeBezirkeAachen.shp > data/aachen_net/aachen_district_map.shp
  unzip -p data/aachen_net/district_map.zip StatistischeBezirkeAachen.shx > data/aachen_net/aachen_district_map.shx
  unzip -p data/aachen_net/district_map.zip StatistischeBezirkeAachen.dbf > data/aachen_net/aachen_district_map.dbf
  unzip -p data/aachen_net/district_map.zip StatistischeBezirkeAachen.prj > data/aachen_net/aachen_district_map.prj

  # gather NRW roads
  unzip -p data/aachen_net/NRW_map.zip gis_osm_roads_free_1.shp > data/aachen_net/NRW_roads.shp
  unzip -p data/aachen_net/NRW_map.zip gis_osm_roads_free_1.shx > data/aachen_net/NRW_roads.shx
  unzip -p data/aachen_net/NRW_map.zip gis_osm_roads_free_1.dbf > data/aachen_net/NRW_roads.dbf

  # gather NRW buildings
  unzip -p data/aachen_net/NRW_map.zip gis_osm_buildings_a_free_1.shp > data/aachen_net/NRW_buildings.shp
  unzip -p data/aachen_net/NRW_map.zip gis_osm_buildings_a_free_1.shx > data/aachen_net/NRW_buildings.shx
  unzip -p data/aachen_net/NRW_map.zip gis_osm_buildings_a_free_1.dbf > data/aachen_net/NRW_buildings.dbf
#+END_SRC

* Create city map
Put all relevant NRW data inside a ~postgresql~ database with ~postgis~ extension installed.
Database is run localy as normal user inside a proper socket directory.

See [[https://simonb83.github.io/making-a-map-in-matplotlib.html][here]] for details on moving ~shp~ to ~postgresql~.

#+NAME: socket_dir
- data/aachen_net/postgres/socket_dir/

#+BEGIN_SRC bash :results none :tangle scripts/aachen_net/02_postgres_init.sh :var socket_dir=socket_dir
  # create and start local postgres session
  mkdir -p data/aachen_net/postgres/
  initdb -D data/aachen_net/postgres/

  mkdir -p $(pwd)/$socket_dir
  postgres -D data/aachen_net/postgres/ -k $(pwd)/$socket_dir &

  dropdb nrw -h $(pwd)/$socket_dir
  createdb nrw -h $(pwd)/$socket_dir
  psql nrw -c 'CREATE EXTENSION postgis' -h $(pwd)/$socket_dir

  echo "WARNING: this takes some time..."

  shp2pgsql -s 4326 data/aachen_net/NRW_roads.shp roads | psql nrw -h $(pwd)/$socket_dir > /dev/null
  shp2pgsql -s 4326 data/aachen_net/NRW_buildings.shp buildings | psql nrw -h $(pwd)/$socket_dir > /dev/null
#+END_SRC

** Retrieve city border
Extract the union of Aachen metropolitan area, in order to filter roads properly.

#+BEGIN_SRC python :results none :noweb yes :tangle scripts/aachen_net/03_city_border.py
  <<imports_&_defaults>>
  <<projection>>
  <<districts>>

   with open('data/aachen_net/aachen_border.txt', 'w') as outfile:
       # extract border
       aachen_area = cascaded_union(district_map['geometry'])

       # convert back to (lat, long) for this purpose
       aachen_area = Polygon([projection(*coord[0:2], inverse=True) \
                              for coord in aachen_area.exterior.coords])

       # convert border from 3D to 2D
       outfile.write(aachen_area.to_wkt())
#+END_SRC

** Extract city information
Create output ~shp~ for roads and buildings using the following queries.

#+NAME: roads_query
#+BEGIN_SRC sql
  SELECT osm_id, geom FROM roads
   WHERE fclass NOT IN ('trunk_link', 'bridleway', 'motorway',
                        'motorway_link', 'path', 'primary_link',
                        'secondary_link', 'service', 'steps',
                        'tertiary_link', 'track', 'track_grade2',
                        'track_grade3', 'track_grade4', 'track_grade5',
                        'unclassified', 'unknown')
     AND ST_Intersects(geom, ST_SetSRID(ST_GeomFromText('$aachen_border'), 4326));
#+END_SRC

#+NAME: buildings_query
#+BEGIN_SRC sql
  SELECT osm_id, geom, type FROM buildings
   WHERE ST_Intersects(geom, ST_SetSRID(ST_GeomFromText('$aachen_border'), 4326));
#+END_SRC

#+BEGIN_SRC bash :noweb yes :results output :tangle scripts/aachen_net/04_extraction.sh :var socket_dir=socket_dir
  # extract roads around aachen border
  read aachen_border < data/aachen_net/aachen_border.txt

  # due to noweb shortcomings, first newline and leading whitespaces have to be removed
  query="
         <<roads_query>>"
  pgsql2shp -f data/aachen_net/aachen_roads -h $(pwd)/$socket_dir nrw \
            "$(echo ${query:1:${#query}} | sed 's/^[\t ]*//g')"

  query="
         <<buildings_query>>"
  pgsql2shp -f data/aachen_net/aachen_buildings -h $(pwd)/$socket_dir nrw \
            "$(echo ${query:1:${#query}} | sed 's/^[\t ]*//g')"
#+END_SRC

** Plot the map
Create final plot, with roads as well as population density.

#+BEGIN_SRC python :results none :noweb yes :tangle scripts/aachen_net/05_plot_map.py :var valid_types=flatten(utils.org:valid_types)
  <<all_datasets>>

  fig = plt.figure(figsize=(6, 6), frameon=False)
  ax = fig.gca()

  district_map.plot(column='density',
                    cmap='viridis',
                    legend=True,
                    linewidth=0, # remove districts borders
                    alpha=0.7,
                    ax=ax)

  roads_map.plot(color='black',
                 alpha=0.6,
                 linewidth=0.15,
                 ax=ax)

  buildings_map.plot(color='black',
                     alpha=0.5,
                     linewidth=0.15,
                     ax=ax)

  # set title of colorbar (dirty trick)
  fig.get_axes()[1].set_title('    people/kmÂ²',
                              fontweight=font_spec['font.weight'],
                              fontsize=12)

  fig.get_axes()[1].tick_params(labelsize=10)

  plt.axis('off')
  plt.tight_layout(rect=[-1.25, -0.04, 1.2, 1])

  # plt.show()

  plt.savefig('figures/aachen_citymap.png', dpi=250)
  plt.close('all')
#+END_SRC

* Create graph from city roads
** Create an abstract graph from the roads
When creating the graph, remember ~OSM_ID~, in order to assign each building a proper edge.

#+NAME: graph_path
- data/aachen_net/aachen_graph

#+BEGIN_SRC python :noweb yes :var valid_types=flatten(utils.org:valid_types) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/06_get_roads_graph.py
  <<imports_&_defaults>>
  <<projection>>
  <<roads>>
  <<buildings>>

  sg = ShapeGraph(shapefile=roads_path, to_graph=True, properties=['OSM_ID'])

  # convert graph to json
  G = json_graph.node_link_data(sg.graph)

  for node in G['nodes']:
      node['lat'], node['lon'] = sg.node_xy[node['id']]

  # use of private variable seems to be mandatory here
  edge_osm_id_map = {
      edge: sg.line_info(info.line_index).props['OSM_ID']
      for edge, info in sg._edges.items() if info.line_index is not None
  }

  for edge in G['edges']:
      if edge in edge_osm_id_map:
          G[edge[0]][edge[1]]['OSM_ID'] = edge_osm_id_map[edge]

  with open(graph_path + "_0_raw.json", 'w') as output:
      output.write(json.dumps(G))
#+END_SRC

** Adjust road length
For our purposes, roads should have a maximum length of 200m and a minimum of
20m, in order to make our setting buildings in street corners accurate enough.

#+NAME: MIN_LENGTH
- 20

#+NAME: MAX_LENGTH
- 200

#+BEGIN_SRC python :noweb yes :tangle scripts/aachen_net/07_fix_short_roads.py :var graph_path=flatten(graph_path) :var min_length=flatten(MIN_LENGTH)
  <<imports_&_defaults>>

  # load graph
  with open(graph_path + "_0_raw.json") as f:
      js_graph = json.load(f)

  G = json_graph.node_link_graph(js_graph)

  assert nx.is_connected(G), "Raw G is not connected"

  ## remove too short roads

  MIN_LENGTH = int(min_length)

  def order_edge(edge):
      return min(edge), max(edge)

  # precompute expensive distance dictionary (update each cycle)
  edge_length_map = { order_edge(edge): node_distance(G, *edge)
                      for edge in G.edges() }

  # proceed splitting all roads that are shorter than MIN_LENGTH
  while True:
      current_min_length = float('inf')
      min_source = None
      min_target = None

      n = 0
      # compute length of each road
      for edge, length in edge_length_map.items():
          # keep track of the shortest road
          if length < current_min_length:
              current_min_length = length
              min_source, min_target = edge

          # count how many are still there
          if length < MIN_LENGTH:
              n += 1

      if current_min_length > MIN_LENGTH:
          break

      # segment from min_source to min_target
      min_g = Geodesic.WGS84.Inverse(
          G.node[min_source]['lat'], G.node[min_source]['lon'],
          G.node[min_target]['lat'], G.node[min_target]['lon']
      )

      # use mid-point for contracted node position
      mid_point = Geodesic.WGS84.Direct(lat1= G.node[min_source]['lat'],
                                        lon1= G.node[min_source]['lon'],
                                        azi1= min_g['azi1'],
                                        s12=  min_g['s12']/2)

      # new edges from min_target will be from min_source
      # work on (min_target, ...) but avoid (min_target, min_source)
      new_edges = [ (min_source, w)
                    for x, w in G.edges(min_target)
                    if w != min_source ]

      # remove edges touching min_target from the lengths dictionary
      for edge in G.edges(min_target):
          del edge_length_map[ order_edge(edge) ]

      # remove node and its edges and add new ones
      G.remove_node(min_target)
      G.add_edges_from(new_edges)

      # move node to keep in the middle point
      G.node[min_source].clear()
      G.node[min_source]['lat'] = mid_point['lat2']
      G.node[min_source]['lon'] = mid_point['lon2']

      # min_source has moved: recompute distances for each edge
      for edge in G.edges(min_source):
          edge_length_map[ order_edge(edge) ] = node_distance(G, *edge)

      # compute distances for each of the new edges
      for edge in new_edges:
          edge_length_map[ order_edge(edge) ] = node_distance(G, *edge)

      print('{} remaining'.format(n - 1), end="\r")

  # check if operation was successful
  refresh_distances(G)
  assert min(data['length'] for _, _, data in G.edges(data=True)) >= MIN_LENGTH

  assert nx.is_connected(G), "Intermediate G is not connected"

  with open(graph_path + "_1_temp.json", 'w') as output:
      output.write(json.dumps(json_graph.node_link_data(G)))
#+END_SRC

Use saved data (with roads /long/ enough) to start working on the ones too long.

#+BEGIN_SRC python :noweb yes :tangle scripts/aachen_net/08_fix_long_roads.py :var graph_path=flatten(graph_path) :var min_length=flatten(MIN_LENGTH) :var max_length=flatten(MAX_LENGTH)
  <<imports_&_defaults>>

  # load graph
  with open(graph_path + "_1_temp.json", "r") as f:
      js_graph = json.load(f)

  G = json_graph.node_link_graph(js_graph)

  assert nx.is_connected(G), "Raw G is not connected!"

  ## split roads that are too long

  MIN_LENGTH = int(min_length)
  MAX_LENGTH = int(max_length)

  # collect edges (not to mess up with G iterator)
  edges_to_split_distance = { edge: node_distance(G, *edge)
                              for edge in G.edges()
                              if node_distance(G, *edge) >= MAX_LENGTH }

  progress = 1
  for (source, target), distance in edges_to_split_distance.items():
      print("{}/{} roads splitted".format(progress, len(edges_to_split_distance)), end='\r')
      progress += 1

      G.remove_edge(source, target)

      # number of new segments
      n_segments = int(ceil(distance / MAX_LENGTH))

      # n + source + target now are in the segment
      delta = distance / n_segments

      if delta > MAX_LENGTH:
          print("Nope", delta)
          exit(1)

      # run along segment from source to target
      g = Geodesic.WGS84.Inverse(
          G.node[source]['lat'], G.node[source]['lon'],
          G.node[target]['lat'], G.node[target]['lon']
      )

      new_points = []
      for i in range(1, n_segments):
          # disseminate points every delta
          point = Geodesic.WGS84.Direct(lat1= G.node[source]['lat'],
                                        lon1= G.node[source]['lon'],
                                        azi1= g['azi1'],
                                        s12=  delta * i)

          new_points.append(max(G.nodes) + 1)
          G.add_node(max(G.nodes) + 1,
                     lat=point['lat2'],
                     lon=point['lon2'])

      G.add_edge(source, new_points[0])

      for j in range(n_segments - 2):
          G.add_edge(new_points[j], new_points[j+1])

      G.add_edge(new_points[-1], target)

  # check distances respect the constraints
  refresh_distances(G)
  assert max(data['length'] for _, _, data in G.edges(data=True)) <= MAX_LENGTH, "Max length exceeded"
  assert min(data['length'] for _, _, data in G.edges(data=True)) >= MIN_LENGTH, "Min length not respected"

  assert nx.is_connected(G), "Processed G not connected!"

  with open(graph_path + "_1_fix_roads.json", 'w') as output:
      output.write(json.dumps(json_graph.node_link_data(G)))
#+END_SRC

Find the closest road on the map for each house, in order to set the house as a node on the road graph.
This has to consider only roads in the major component of the city, not the unconnected ones.

** Assign buildings to nodes
#+NAME: closest_nodes_path
- data/aachen_net/closest_nodes.csv

Load data from buildings dataset into graph.

#+BEGIN_SRC python :noweb yes :tangle scripts/aachen_net/09_add_buildings.py :var valid_types=flatten(utils.org:valid_types) :var closest_nodes_path=flatten(closest_nodes_path) :var graph_path=flatten(graph_path)
  <<all_datasets>>

  # load graph
  with open(graph_path + "_1_fix_roads.json") as f:
      js_graph = json.load(f)

  G = json_graph.node_link_graph(js_graph)

  assert nx.is_connected(G), "Fixed roads G is not connected"

  ## filter out buildings, heuristically

  # remove buildings that are too big or too small to be residential
  buildings_map = buildings_map[ (buildings_map.area > 40) &
                                 (buildings_map.area < 2000) ]

  ## assign each building area and district to a node

  # pre-compute all (projected) node points with scipy.spatial.KDTree
  node_ids = list(G.nodes)
  node_coords = list(projection(data['lon'], data['lat'])
                     for _, data in G.nodes(data=True))
  search_tree = spatial.KDTree(node_coords)

  building_distances = []

  building_index = 0
  for _, building in buildings_map.iterrows():
      if building_index % 200 == 0:
          print("{}/{}".format(building_index, len(buildings_map)), end='\r')
      building_index += 1

      ## work only if building can be assigned to a district (not ones outside
      ## city)

      district_index = -1
      for index, district_row in district_map.iterrows():
          if building.geometry.centroid.within(district_row.geometry):
              district_index = index

      # avoid adding buildings which center is outside the city
      if district_index == -1:
          continue

      ## find closest point in the graph

      _, min_node_index = search_tree.query( (building.geometry.centroid.x,
                                              building.geometry.centroid.y) )
      node_id = node_ids[min_node_index]

      # measure building -> node distance precisely
      building_lon, building_lat = projection(building.geometry.centroid.x,
                                              building.geometry.centroid.y,
                                              inverse=True)

      node_lon, node_lat = projection(*node_coords[min_node_index], inverse=True)

      min_dist = compute_distance({'lon': building_lon, 'lat': building_lat},
                                  {'lon': node_lon,     'lat': node_lat    })

      ## register value both in error measurer and graph

      building_distances.append(min_dist)

      node_data = G.node[node_id]

      # fill the structures if needed
      if 'district_count' not in node_data:
          node_data['district_count'] = {}

      if district_index not in node_data['district_count']:
          node_data['district_count'][district_index] = 0

      # update values for the node: each building contributes with its area to
      # the district count, in order to have

      # 1) total building area assigned to the node
      # 2) voting on district (based on area) to assigned the node to a district
      node_data['district_count'][district_index] += building.geometry.area

  with open('data/aachen_net/buildings_position_error.csv', 'w') as f:
      for dist in building_distances:
          f.write("{}\n".format(dist))

  with open(graph_path + "_2_temp.json", 'w') as output:
      output.write(json.dumps(json_graph.node_link_data(G)))
#+END_SRC

Based on area and district metric, assign each node a number of internet customer lines.

#+BEGIN_SRC python :noweb yes :tangle scripts/aachen_net/10_compute_n_lines.py :var closest_nodes_path=flatten(closest_nodes_path) :var graph_path=flatten(graph_path)
  <<imports_&_defaults>>
  <<districts>>
  <<graph_utils>>
  <<population>>

  with open(graph_path + "_2_temp.json") as f:
      js_graph = json.load(f)

  G = json_graph.node_link_graph(js_graph)

  ## assign the district by majority vote on area

  # nodes with no area have not to be assigned any district
  for node_id in G.nodes():
      node_data = G.node[node_id]

      node_data['area'] = 0
      node_data['district'] = None

      # override values if needed
      if 'district_count' in node_data:
          area_count = node_data['district_count']

          node_data['area'] = sum(area_count.values())
          node_data['district'] = max(area_count, key=lambda x: area_count[x])

          del node_data['district_count']

  ## split population across all nodes in the same district

  # compute total building area per district
  district_area_map = { id_: 0 for id_ in district_map.index }
  for node_id, data in G.nodes(data=True):
      if data['district']:
          district_area_map[int(data['district'])] += data['area']

  # distribute population accordingly
  for node_id in G.nodes():
      node_data = G.node[node_id]

      if node_data['district']:
          district_id = int(node_data['district'])

          node_data['population'] = \
              node_data['area'] / \
              district_area_map[district_id] * \
              district_map.loc[district_id].population
      else:
          node_data['population'] = 0

      del node_data['area']
      del node_data['district']

  ## this section is /completely/ heuristic! done to match probable (supposed)
  ## numbers about Aachen network

  # compute number of lines per node, given assigned population
  for node_id in G.nodes():
      node_data = G.node[node_id]

      n_lines = int(node_data['population'] // 6)

      # avoid number of lines too small or too big
      if n_lines < 1:
          node_data['n_lines'] = 0
      elif n_lines > 48:
          node_data['n_lines'] = 48
      else:
          node_data['n_lines'] = n_lines

      del node_data['population']

  with open(graph_path + "_2_added_buildings.json", 'w') as output:
      output.write(json.dumps(json_graph.node_link_data(G)))

  # since this is the last process, just save it as "complete"
  # allow painless conversion to GraphML format
  for _, data in G.nodes(data=True):
      data_copy = data.copy()
      data.clear()

      data['lon'] = float(data_copy['lon'])
      data['lat'] = float(data_copy['lat'])
      data['n_lines'] = int(data_copy['n_lines'])

      # compatibility data for plot
      data['active'] = True
      data['is_subroot'] = False

  for _, _, data in G.edges(data=True):
      data_copy = data.copy()
      data.clear()

      data['length'] = float(data_copy['length'])

      # compatibility data for plot
      data['active'] = True

  convert_properties(G, str)
  nx.write_graphml(G, graph_path + "_complete.graphml")
#+END_SRC

** Plot obtained graph on the map
Plot final graph on top of district map.

#+BEGIN_SRC python :noweb yes :var valid_types=flatten(utils.org:valid_types) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/11_plot_city_graph.py
  <<imports_&_defaults>>
  <<projection>>
  <<districts>>
  <<graph_utils>>
  <<plot_result>>

  # read graph G
  G = nx.read_graphml(graph_path + "_complete.graphml")
  convert_properties(G, float)

  ## plot everything
  fig = plt.figure(figsize=(6, 5.6), frameon=False)
  ax = fig.gca()

  plot_result(G,
              ax=ax,
              title="Detail of extracted city graph\n",
              projection=projection)

  # plot just city external border (already projected)
  aachen_border = cascaded_union(district_map.geometry)
  # remove hole in union
  aachen_border = aachen_border.buffer(100).buffer(-100)

  gpd.GeoDataFrame({'geometry': [aachen_border]}).plot(color='white',
                                                       edgecolor='black',
                                                       linewidth=0.5,
                                                       ax=ax,
                                                       zorder=-2)

  plt.subplots_adjust(top=0.85,
                      bottom=0.0,
                      left=0.0,
                      right=1.0)

  plt.savefig('figures/aachen_city_graph.png', dpi=250)
  plt.close('all')

  # plt.show()
#+END_SRC

#+RESULTS:

* Solve optimization problem
Solve the optimization problem in two ways, exactly with ILP and through an
heuristic algorithm.

** Exact solution with ILP
See notes file for mathematical model implemented here.

Since the very same procedure has to be applied at the various levels of the
tree, the model definition is wrapped in a function.

#+NAME: solver_ILP
#+BEGIN_SRC python
  import csv
  import json
  import logging
  import math
  from math import sqrt
  from pathlib import Path

  import networkx as nx
  from networkx.readwrite import json_graph

  import cplex
  from docplex.mp.model import Model

  logger = logging.getLogger('aachen_net.org')
  logger.setLevel(logging.INFO)
  logger.propagate = False

  formatter = logging.Formatter("%(asctime)s::%(levelname)s::%(module)s::%(message)s",
                                "%Y-%m-%d %H:%M:%S")

  ch = logging.StreamHandler()
  ch.setLevel(logging.INFO)
  ch.setFormatter(formatter)
  logger.addHandler(ch)

  logger.info('import ok')

  def ILP_solve(G, params, logfile="logs/unnamed.log", minimum_gap=1e-4):
      '''
      G needs to have this properties
      - n_lines != 0 only for active nodes
      - distance for all edges

      Output will provide another graph with
      - edges
        - x, n: activity flag and number of lines passing
        - active: activity boolean (as heuristic)
        - n_lines: number of lines served of subroot (0 if not subroot)
      - nodes
        - d: distance from subroot (if active)
        - is_subroot: flag for subroots
        - active: activity boolean
      '''

      #####################
      # Pre-process graph #
      #####################
      G = G.to_directed()

      # add artificial root node to G, with a zero-length arc for all the nodes
      G.add_node('r', n_lines=0, lat=-1, lon=0)

      for node_id in G.nodes:
          # TODO check if node is in R (candidate sub-roots)
          G.add_edge('r', node_id, length=0)

      ###################
      # Setup variables #
      ###################

      m = Model(log_output=True)
      m.parameters.mip.tolerances.mipgap = minimum_gap
      m.parameters.workmem = 2048
      m.parameters.mip.display = 2
      m.parameters.mip.interval = -1

      def name(source, target=None, var='x'):
          if target:
              return "{}_{}~{}".format(var, source, target)
          else:
              return "{}_{}".format(var, source)

      X = {}
      N = {}
      for i, (source, target) in enumerate(G.edges):
          if i % 10000 == 0:
              print("Initializing edge {}/{}".format(i, len(G.edges)), end='\r')

          if source not in X:
              X[source] = {}

          if source not in N:
              N[source] = {}

          ## active edge indicator
          X[source][target] = m.binary_var(name=name(source, target, var='x'))
          N[source][target] = m.integer_var(name=name(source, target, var='n'))
          m.add_constraint(ct=N[source][target] >= 0,
                           ctname="{} >= 0".format(name(source, target, var='n')))

      D = {}
      for i, (node_id, data) in enumerate(G.nodes(data=True)):
          if i % 10000 == 0:
              print("Initializing node {}/{}".format(i, len(G.nodes)), end='\r')

          ## set distance counter
          D[node_id] = m.continuous_var(name=name(node_id, var='d'))
          m.add_constraint(ct=D[node_id] >= 0,
                           ctname="{} >= 0".format(name(source, target, var='d')))

      logger.info('Initialized variables')

      ######################
      # Objective function #
      ######################

      obj_func = 0

      # A), B) ~> suppose full-fiber for now
      for node_id, data in G.nodes(data=True):
          obj_func += D[node_id] * data['n_lines'] * params['c_f']

      # C)
      for source, target, data in G.edges(data=True):
          obj_func += X[source][target] * data['length'] * params['c_e']

      # D)
      for source, target in G.out_edges('r'):
          obj_func += X[source][target] * params['c_r']

      m.set_objective('min', obj_func)

      logger.info('Initialized objective function')

      ###############
      # Constraints #
      ###############

      for i, (node_id, data) in enumerate(G.nodes(data=True)):
          if i % 1000 == 0:
              print("Constraints on node {}/{}".format(i, len(G.nodes)), end='\r')

          in_count_X = 0
          in_count_N = 0
          for source, target in G.in_edges(node_id):
              in_count_X += X[source][target]
              in_count_N += N[source][target]

          out_count_X = 0
          out_count_N = 0
          for source, target in G.out_edges(node_id):
              out_count_X += X[source][target]
              out_count_N += N[source][target]

          # 2)
          if node_id == 'r':
              m.add_constraint(ct=in_count_X == 0,
                               ctname=name(node_id, var='in_count_X'))
              # terminal node
          elif data['n_lines'] > 0:
              m.add_constraint(ct=in_count_X == 1,
                               ctname=name(node_id, var='in_count_X'))
          else:
              m.add_constraint(ct=in_count_X <= 1,
                               ctname=name(node_id, var='in_count_X'))

          # 3)
          if node_id == 'r':
              m.add_constraint(ct=out_count_X >= 1,
                               ctname=name(node_id, var='out_count_X_lower'))

          # 4)
          m.add_constraint(ct=D[node_id] <= in_count_X * params['d_M'],
                           ctname="{}".format(name(node_id, var='distance_domain')))

          # 7), 8)
          if node_id != 'r':
              m.add_constraint(ct=in_count_N - out_count_N == data['n_lines'],
                               ctname="{}".format(name(node_id, var='flow_balance')))
          else:
              total_population = sum(data_['n_lines'] for _, data_ in G.nodes(data=True))
              m.add_constraint(ct=out_count_N == total_population,
                               ctname="{}".format(name(node_id, var='flow_balance')))

      logger.info('Set constraints 2, 3, 4, 7, 8')

      for i, (source, target, data) in enumerate(G.edges(data=True)):
          if i % 1000 == 0:
              print("Constraints on edge {}/{}".format(i, len(G.edges)), end='\r')

          edge_length = data['length']

          # 5)
          m.add_constraint(ct=D[target] - D[source] >= edge_length * X[source][target] - params['d_M'] * (1 - X[source][target]),
                           ctname="{}".format(name(source, target, var='distance_upper')))

          m.add_constraint(ct=D[target] - D[source] <= edge_length * X[source][target] + params['d_M'] * (1 - X[source][target]),
                           ctname="{}".format(name(source, target, var='distance_lower')))

          # 6)
          m.add_constraint(ct=N[source][target] <= params["n_M"] * X[source][target],
                           ctname="{}".format(name(source, target, var='n_max')))

      logger.info('Set constraints 5, 6')

      with open(logfile, "w") as f:
          solver = m.solve(log_output=f)

      if solver is None:
          logger.error("Unable to solve ILP")
          exit(1)
      else:
          # m.print_solution()
          pass

      # save results to graph
      for source, target in G.edges:
          G[source][target]['x'] = X[source][target].solution_value
          G[source][target]['n'] = N[source][target].solution_value

      for node_id in G.nodes:
          G.node[node_id]['d'] = D[node_id].solution_value

      # allow painless conversion to GraphML format
      for node_id, data in G.nodes(data=True):
          data_copy = data.copy()
          data.clear()

          data['d'] = float(data_copy['d'])
          data['lon'] = float(data_copy['lon'])
          data['lat'] = float(data_copy['lat'])

          # mark active and subroot nodes
          data['active'] = data_copy['n_lines'] > 0

          ## NOTE that n_lines change meaning, as current terminals will be ignored in further iterations in favour of current sub-roots

          # check subroots
          if G['r'][node_id]['x'] == 1:
              data['is_subroot'] = True
              data['n_lines'] = int(N['r'][node_id])
          else:
              data['is_subroot'] = False
              data['n_lines'] = 0

      # remove artificial root node
      G.remove_node('r')

      for _, _, data in G.edges(data=True):
          data_copy = data.copy()
          data.clear()

          data['length'] = float(data_copy['length'])
          data['x'] = int(data_copy['x'])
          data['n'] = int(data_copy['n'])

          # mark active edges
          data['active'] = data['x'] == 1

      return G
#+END_SRC

** Heuristic approach
My own devised heuristic approach requires, in order to run properly some
utility functions, listed here.

This one creates and stores in cache the pairwise distance matrix for all
terminal (or active) nodes.

#+NAME: terminal_dist_cache
#+BEGIN_SRC python
  def get_vertex_distance(G, cache_path=None, clear_cache=False):
      if cache_path is None:
          cache_path = Path('data/aachen_net/distance_matrix_temp.h5')

      # this takes some time: use cache file
      if not cache_path.exists() or clear_cache:
          vertex_dist = shortest_distance(G, weights=G.edge_properties['length'])

          with h5py.File(cache_path, 'w') as f:
              # obtain which and how many nodes have to be considered terminals
              N = G.num_vertices()

              # fill matrix with their pairwise distances
              matrix = f.create_dataset('vertex_dist', (N, N))
              for index, vertex_id in enumerate(G.vertices()):
                  if index % 100 == 0:
                      print("Filling distance matrix", index, "/", N, end='\r')

                  matrix[index, :] = np.array(vertex_dist[vertex_id])

      with h5py.File(cache_path, 'r') as f:
          # keep track of distance among vertices
          vertex_dist = np.array(f['vertex_dist'])

      # check simmetry
      assert np.allclose(vertex_dist, vertex_dist.T, equal_nan=True), \
          "Distance matrix not symmetric"

      return vertex_dist
#+END_SRC

The function ~get_min_path_tree~ finds the cheapest minimum path tree,
measuring its cost for each node of the cluster as root.

#+NAME: min_path_tree
#+BEGIN_SRC python
  paths = {}

  def _order(i, j):
      return tuple(sorted([i, j]))

  def compute_path(G, i, j, enable_cache=False):
      global paths

      if enable_cache:
          if _order(i, j) in paths:
              return paths[_order(i, j)]

      sp_edges = shortest_path(G, i, j)[1]
      ids = [G.edge_index[e] for e in sp_edges]

      # mark edges of the path and store the total path length
      mask = np.zeros(G.num_edges(), dtype=bool)
      mask[ids] = True

      edge_count = np.zeros(G.num_edges(), dtype=np.int)
      mask[ids] = 1

      if enable_cache:
          paths[_order(i, j)] = (mask, edge_count)

      return (mask, edge_count)

  def get_min_path_tree(G, vertex_dist, cluster_nodes, is_terminal, params, fast_mode=False, enable_path_cache=True):
      # compute pairwise path for each couple of nodes in the cluster
      # and remove duplicate edges
      best_road_length = float('inf')
      best_root = None
      best_tree_edges = None

      # this tracks cable length in the best configuration *for the roads*:
      # choices had to be made to make computation fast
      best_cable_length = float('inf')

      # since root can be any node, explore non_terminal nodes also as candidate
      # in a neighbourhood. Top do so, look at the rows of vertex_dist
      # corresponding to the cluster, then pick all nodes (column indices) that
      # are close enough
      if fast_mode:
          neighbour_nodes = cluster_nodes
      else:
          _, neighbour_nodes = np.where(
              vertex_dist[cluster_nodes, :] <= params['discovery_dist']
          )

      for index, root in enumerate(set(neighbour_nodes)):
          # print("root {}/{}".format(index, len(set(neighbour_nodes))), end="\r")

          # compute edges of tree with root node
          tree_edges = np.zeros(G.num_edges(), dtype=bool)
          cable_edge_counts = np.zeros(G.num_edges(), dtype=np.int)

          current_cable_length = 0

          # tree from root has to reach all terminals
          for node in cluster_nodes:
              path_edges, edge_count = compute_path(G,
                                                    G.vertex(root),
                                                    G.vertex(node),
                                                    enable_cache=enable_path_cache)

              # a cable has to be placed per each customer
              cable_edge_counts += G.vp['n_lines'][G.vertex(node)] * edge_count
              tree_edges = np.logical_or(tree_edges, path_edges)

          current_road_lengths = G.ep['length'].a * tree_edges
          current_road_length = np.sum(current_road_lengths)

          if np.all(current_road_lengths) < params['d_M'] and \
             current_road_length < best_road_length:
              best_road_length  = current_road_length
              best_cable_length = np.sum(cable_edge_counts)
              best_root         = root
              best_tree_edges   = tree_edges

      if best_root is None:
          print("Error, root is None")
          exit(1)

      return best_road_length, best_cable_length, best_root, best_tree_edges
#+END_SRC

Heuristics simply tries to merge groups of nodes together until it is no longer
economically viable. Starting with singleton clusters of terminals, each round
the two closest ones are merged, if possible, and otherwise removed from the
search.

Merge has to guarantee that
- diamater limit $d_M$ is not exceeded for the new group
- number of lines is less than $n_M$

#+NAME: solver_heuristic
#+BEGIN_SRC python
  def update_metric(clusters, X, i, j, func=np.nanmax):
      i_idx = np.where(clusters == clusters[i])[0]
      j_idx = np.where(clusters == clusters[j])[0]

      X[i_idx, :] = func( (X[i, :], X[j, :]), axis=0 )
      X[j_idx, :] = X[i, :]

      X[:, i_idx] = X[i_idx, :].T
      X[:, j_idx] = X[j_idx, :].T

  def disable_link(clusters, X, i, j):
      i_idx = np.where(clusters == clusters[i])[0]
      j_idx = np.where(clusters == clusters[j])[0]

      cmn_idx = np.hstack((i_idx, j_idx))
      X[np.ix_(cmn_idx, cmn_idx)] = np.nan

  def objective_function(G, vertex_dist, clusters, is_terminal, params):
      # sub-root cost
      sub_root_cost = len(set(clusters)) * params['c_r']

      vertices = np.array([int(v) for v in G.vertices()])
      terminals = vertices[is_terminal]

      # use minimum path tree also for excavation (remove duplicate edges)
      total_cable_length = 0
      total_road_length = 0
      for index, cluster_id in enumerate(set(clusters)):
          cluster_nodes = terminals[clusters == cluster_id]

          # deal with simplest cases, avoiding MST computation
          if len(cluster_nodes) == 1:
              continue

          road_length, cable_length, _, _  = get_min_path_tree(G,
                                                               vertex_dist,
                                                               cluster_nodes,
                                                               is_terminal,
                                                               params,
                                                               fast_mode=True,
                                                               enable_path_cache=True)

          total_road_length += road_length
          total_cable_length += cable_length

      excavation_cost = total_road_length  * params['c_e']
      cable_cost      = total_cable_length * params['c_f']

      return sub_root_cost + cable_cost + excavation_cost

  def heuristic_solve(G, params):
      global paths

      if isinstance(G, nx.DiGraph) or isinstance(G, nx.Graph):
          G = nx2gt(G)

      vertices = np.array([int(v) for v in G.vertices()])
      N = G.num_vertices()

      ## save information about terminals
      n_lines = np.array([G.vp['n_lines'][v] for v in G.vertices()], dtype=np.int)
      is_terminal = n_lines > 0

      terminals = vertices[is_terminal]
      terminal_lines = n_lines[is_terminal]

      T = len(terminals)

      # initialize clusters: at the beginning they are singletons of terminals
      clusters = terminals.copy()
      cluster_lines = terminal_lines.copy()

      vertex_dist = get_vertex_distance(G)
      terminal_dist = vertex_dist[np.ix_(is_terminal, is_terminal)]
      min_cluster_dist = terminal_dist.copy()

      initial_total_cluster_lines = sum(cluster_lines)

      # fill diagonal with nans (values we want to ignore)
      np.fill_diagonal(min_cluster_dist, np.nan)

      # create a matrix for the max distance between nodes in two different clusters
      # bij = diameter of cluster obtained joining i-th and j-th ones
      max_cluster_dist = min_cluster_dist.copy()

      # # prune the nodes that even at the beginning are further from each other more
      # # than the critical length
      min_cluster_dist[min_cluster_dist > params['d_M']] = np.nan
      max_cluster_dist[min_cluster_dist > params['d_M']] = np.nan

      total_couples = np.count_nonzero( np.isfinite(min_cluster_dist)) / 2
      logger.info("Pruning: left {:.0f} out of {}".format(total_couples, T * (T-1) // 2))

      # counters
      n_iter = 0
      previous_total_cost = float('inf')
      best_clusters = None

      while True:
          n_iter += 1

          # stop if all couples have been checked
          if np.all(np.isnan(min_cluster_dist)):
              logger.info("Checked all possible couples")
              break

          if n_iter % 50 == 0:
              total_cost = objective_function(G,
                                              vertex_dist,
                                              clusters,
                                              is_terminal,
                                              params)

              logger.info("Money {:.2f}Mâ¬ n_cluster {} cache_size {}".format(total_cost/1e6,
                                                                             len(set(clusters)),
                                                                             len(paths)))

              # stop if the found solution is no better than the previous one
              if total_cost > previous_total_cost:
                  logger.info("Minimum exceeded")
                  clusters = best_clusters
                  break

              else:
                  # update best solution with current one
                  best_clusters = clusters.copy()
                  previous_total_cost = total_cost

          # get two closest clusters (heuristic measure)
          min_rows, min_cols = np.where(min_cluster_dist == np.nanmin(min_cluster_dist))

          # i, j are the i-th, j-th cluster
          i, j = min_rows[0], min_cols[0]

          # ensure number of lines and diameter are not exceeded
          total_n_lines = cluster_lines[i] + cluster_lines[j]
          joint_diameter = max_cluster_dist[i][j]

          if total_n_lines <= params['n_M'] and \
             joint_diameter <= 2 * params['d_M']:
              # update merging conditions
              cluster_lines[i] = total_n_lines
              cluster_lines[j] = total_n_lines

              update_metric(clusters, min_cluster_dist, i, j, np.nanmin)
              update_metric(clusters, max_cluster_dist, i, j, np.nanmax)

              # set the same label for the two clusters (minimum given np.where convention)
              new_idx = min(clusters[i], clusters[j])

              clusters[clusters == clusters[i]] = new_idx
              clusters[clusters == clusters[j]] = new_idx

          # since it has been evaluated, remove couple (i, j) from the possibilities:
          # setting their distance to nan
          disable_link(clusters, min_cluster_dist, i, j)

      # save results in the graph
      # create maps if needed
      G.vp['is_subroot'] = G.new_vertex_property("bool")
      G.vp['active']     = G.new_vertex_property("bool")
      G.vp['father_id']  = G.new_vertex_property("int")
      G.ep['active']     = G.new_edge_property("bool")

      G.vp['is_subroot'].a = False
      G.vp['active'].a     = False
      G.vp['father_id'].a  = -1
      G.ep['active'].a     = False

      for index, cluster_id in enumerate(set(clusters)):
          if index % 100 == 0:
              print("Loaded cluster", index, "/", len(set(clusters)), end='\r')

          cluster_nodes = terminals[clusters == cluster_id]
          _, _, root, tree_edges = get_min_path_tree(G,
                                                     vertex_dist,
                                                     cluster_nodes,
                                                     is_terminal, params,
                                                     fast_mode=False,
                                                     enable_path_cache=False)

          # root may also be (in non-fast mode) a non-terminal, so its n_lines
          # would be zero: better set it
          G.vp['n_lines'].a[root] = cluster_lines[ np.where(clusters == cluster_id)[0][0] ]
          G.vp['is_subroot'].a[root] = True
          G.vp['active'].a[cluster_nodes] = True
          G.vp['father_id'].a[cluster_nodes] = root
          G.ep['active'].a = np.logical_or(G.ep['active'].a, tree_edges)

      # make n_lines != only for subroots (terminals of next iteration)
      not_subroot_mask = np.logical_not(G.vp['is_subroot'].a)
      G.vp['n_lines'].a[not_subroot_mask] = 0

      assert initial_total_cluster_lines == G.vp['n_lines'].a.sum(), 'Some lines were lost while merging'

      return G
#+END_SRC

** Plot results
Nice utility for plotting results.
Adaptable to both ILP and heuristic results.

#+NAME: plot_result
#+BEGIN_SRC python
  def plot_result(G,
                  projection=None,
                  cut=True,
                  title=None,
                  ax=None,
                  root_markersize=2,
                  edges_linewidth=0.5):
      # convert to graph_tool if necessary
      if isinstance(G, nx.DiGraph) or isinstance(G, nx.Graph):
          G = nx2gt(G)

      # get geopandas elements to plot
      nodes_df, edges_df = graph_to_geopandas(G,
                                              projection=projection)

      if ax is None:
          fig = plt.figure(figsize=(6, 6), frameon=False)
          ax = fig.gca()

      plot_geopandas_graph(nodes_df[nodes_df['active']],
                           edges_df[edges_df['active']],
                           title=title,
                           ax=ax,
                           root_markersize=root_markersize,
                           edges_linewidth=edges_linewidth)

      edges_df.plot(color='#b2b2b2', zorder=-1, ax=ax, linewidth=edges_linewidth)

      if cut:
          ax.set_xlim(293117, 295351)
          ax.set_ylim(5627800, 5629570)
          plt.tight_layout(rect=[-0.08, 0, 1, 1])
#+END_SRC

* Apply algorithms to the datasets
Problem parameters are of course the same for each level, and are listed in the
proper sections.

** DSLAM positioning
$c_r$, cost per DSLAM has to consider also the cable to reach its corresponding
second level router.
We estimate 1.000â¬ for the DSLAM and 20.000â¬ for the mentioned connection.

See [[https://www.itscosts.its.dot.gov][here]] for pricing. TODO cite as a source

#+NAME: DSLAM_params
| $n_M$ [unit]   |    50 |
| $d_M$ [m]      |  1500 |
| $c_r$ [â¬/unit] | 31000 |
| $c_f$ [â¬/m]    |     3 |
| $c_e$ [â¬/m]    |   100 |

A handy ~org~ function is here provided to ease parameters extraction.

#+NAME: extract
#+BEGIN_SRC python :var model_params=DSLAM_params
  return [(param[1:4], value) for param, value in model_params
          if param != "Parameter"]
#+END_SRC

*** ILP
Try using the ILP solver to the initial dataset.

#+BEGIN_SRC python :noweb yes :var params=extract(DSLAM_params) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/12_solver_ILP_DSLAM.py
  <<solver_ILP>>
  <<convert_properties_nx>>

  # load graph
  G = nx.read_graphml(graph_path + "_complete.graphml")
  convert_properties_nx(G, float)

  # find optimal configuration
  G_prime = ILP_solve(G, dict(params), "logs/DSLAM_ILP.log", 0.02)

  # output to graphml file
  convert_properties_nx(G_prime, str)
  nx.write_graphml(G_prime, graph_path + "_DSLAM_ILP.graphml")
  logger.info("Graph saved to file")
#+END_SRC

Plot detail of obtained graph.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/13_plot_ILP_DSLAM.py
  <<imports_&_defaults>>
  <<projection>>
  <<graph_utils>>
  <<plot_result>>

  G = load_graph(graph_path + "_DSLAM_ILP.graphml")

  plot_result(G,
              projection=projection)

  # plt.show()
  plt.savefig('figures/ILP_DSLAM.png', dpi=250)
  plt.close('all')
#+END_SRC

*** Heuristic
Run the heuristic and save its results as well.

#+BEGIN_SRC python :noweb yes :var params=extract(DSLAM_params) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/14_solver_heuristic_DSLAM.py
  <<imports_&_defaults>>

  # send log also to file
  fh = logging.FileHandler('logs/DSLAM_heuristic.log', mode='w')
  fh.setLevel(logging.INFO)
  fh.setFormatter(formatter)
  logger.addHandler(fh)

  <<projection>>
  <<graph_utils>>
  <<terminal_dist_cache>>
  <<min_path_tree>>
  <<solver_heuristic>>

  np.warnings.filterwarnings('ignore')

  # load graph
  G = load_graph(graph_path + "_complete.graphml")
  convert_properties(G, float)

  for key, value in params:
      logger.info("{}={}".format(key, value))

  # find optimal configuration
  params = dict(params)
  params['discovery_dist'] = 200
  G_prime = heuristic_solve(G, params)

  # output to graphml file
  convert_properties(G_prime, str)
  G_prime.save(graph_path + "_DSLAM_heuristic.graphml")
#+END_SRC

Plot detail of obtained graph.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/15_plot_heuristic_DSLAM.py
  <<imports_&_defaults>>
  <<projection>>
  <<graph_utils>>
  <<plot_result>>
  <<min_path_tree>>

  G = load_graph(graph_path + "_DSLAM_heuristic.graphml")
  convert_properties(G, float)

  plot_result(G,
              projection=projection)

  # plt.show()
  plt.savefig('figures/heuristic_DSLAM.png', dpi=250)
  plt.close('all')
#+END_SRC

** Second level routers positioning
Each 2nd level router is estimated to cost roughly 15.000â¬, and lump cost for
connecting it to the mainframe doubles its price.
Distance $d_M$ is set to a very high value, since in this case it is not relevant.

#+NAME: 2router_params
| $n_M$ [unit]   |   400 |
| $d_M$ [m]      | 150000 |
| $c_r$ [â¬/unit] | 100000 |
| $c_f$ [â¬/m]    |     3 |
| $c_e$ [â¬/m]    |   100 |

*** ILP
#+BEGIN_SRC python :noweb yes :var params=extract(2router_params) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/16_solver_ILP_2router.py
  <<solver_ILP>>
  <<convert_properties_nx>>

  # load graph
  G = nx.read_graphml(graph_path + "_DSLAM_heuristic.graphml")
  convert_properties_nx(G, float)

  # find optimal configuration
  G_prime = ILP_solve(G, dict(params), "logs/2router_ILP.log", 0.02)

  # output to graphml file
  convert_properties_nx(G_prime, str)
  nx.write_graphml(G_prime, graph_path + "_2router_ILP.graphml")
  logger.info("Graph saved to file")
#+END_SRC

Plot detail of obtained graph.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/17_plot_ILP_2router.py
  <<imports_&_defaults>>
  <<projection>>
  <<graph_utils>>
  <<plot_result>>

  G = load_graph(graph_path + "_2router_ILP.graphml")
  convert_properties(G, float)

  plot_result(G,
              projection=projection)

  # plt.show()
  plt.savefig('figures/ILP_2router.png', dpi=250)
  plt.close('all')
#+END_SRC

*** Heuristic
Run the heuristic and save its results as well.

#+BEGIN_SRC python :noweb yes :var params=extract(2router_params) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/18_solver_heuristic_2router.py
  <<imports_&_defaults>>

  # send log also to file
  fh = logging.FileHandler('logs/2router_heuristic.log', mode='w')
  fh.setLevel(logging.INFO)
  fh.setFormatter(formatter)
  logger.addHandler(fh)

  <<projection>>
  <<graph_utils>>
  <<terminal_dist_cache>>
  <<min_path_tree>>
  <<solver_heuristic>>

  np.warnings.filterwarnings('ignore')

  # load graph
  G = load_graph(graph_path + "_DSLAM_heuristic.graphml")
  convert_properties(G, float)

  for key, value in params:
      logger.info("{}={}".format(key, value))

  # dslams (all sub-roots of previous tree) count as 1
  G.vp['n_lines'].a = np.array(G.vp['n_lines'].a > 0, dtype=int)

  # find optimal configuration
  params = dict(params)
  params['discovery_dist'] = 400
  G_prime = heuristic_solve(G, params)

  # output to graphml file
  convert_properties(G_prime, str)
  G_prime.save(graph_path + "_2router_heuristic.graphml")
#+END_SRC

Plot detail of obtained graph.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/19_plot_heuristic_2router.py
  <<imports_&_defaults>>
  <<projection>>
  <<graph_utils>>
  <<plot_result>>
  <<min_path_tree>>

  G = load_graph(graph_path + "_2router_heuristic.graphml")
  convert_properties(G, float)

  plot_result(G,
              projection=projection,
              root_markersize=20)

  # plt.show()
  plt.savefig('figures/heuristic_2router.png', dpi=250)
  plt.close('all')
#+END_SRC

** Mainframe positioning
#+NAME: mainframe_params
| $n_M$ [unit]   | 100000000000 |
| $d_M$ [m]      | 100000000000 |
| $c_r$ [â¬/unit] |            0 |
| $c_f$ [â¬/m]    |            3 |
| $c_e$ [â¬/m]    |          100 |

*** Heuristic
Run the heuristic and save its results as well.

#+BEGIN_SRC python :noweb yes :var params=extract(mainframe_params) :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/20_solver_heuristic_mainframe.py
  <<imports_&_defaults>>

  # send log also to file
  fh = logging.FileHandler('logs/mainframe_heuristic.log', mode='w')
  fh.setLevel(logging.INFO)
  fh.setFormatter(formatter)
  logger.addHandler(fh)

  <<projection>>
  <<graph_utils>>
  <<terminal_dist_cache>>
  <<min_path_tree>>
  <<solver_heuristic>>

  np.warnings.filterwarnings('ignore')

  # load graph
  G = load_graph(graph_path + "_2router_heuristic.graphml")
  convert_properties(G, float)

  for key, value in params:
      logger.info("{}={}".format(key, value))

  vertices = np.array([int(v) for v in G.vertices()])
  N = G.num_vertices()

  ## save information about terminals
  n_lines = np.array([G.vp['n_lines'][v] for v in G.vertices()], dtype=np.int)
  is_terminal = n_lines > 0

  terminals = vertices[is_terminal]
  terminal_lines = n_lines[is_terminal]

  T = len(terminals)

  # initialize clusters: at the beginning they are singletons of terminals
  clusters = terminals.copy()
  cluster_lines = terminal_lines.copy()

  vertex_dist = get_vertex_distance(G)
  terminal_dist = vertex_dist[np.ix_(is_terminal, is_terminal)]

  cluster_nodes = vertices[is_terminal]

  ## find Steiner tree
  params = dict(params)
  params['discovery_dist'] = 1000000

  _, _, root, tree_edges = get_min_path_tree(G,
                                             vertex_dist,
                                             cluster_nodes,
                                             is_terminal,
                                             params,
                                             fast_mode=True,
                                             enable_path_cache=False)

  ## store back to the graph

  G.vp['is_subroot'] = G.new_vertex_property("bool")
  G.vp['active']     = G.new_vertex_property("bool")
  G.ep['active']     = G.new_edge_property("bool")

  G.vp['is_subroot'].a = False
  G.vp['active'].a     = False
  G.ep['active'].a     = False

  # root may also be (in non-fast mode) a non-terminal, so its n_lines
  # would be zero: better set it
  G.vp['is_subroot'].a[root] = True
  G.vp['active'].a[cluster_nodes] = True
  G.ep['active'].a = np.logical_or(G.ep['active'].a, tree_edges)

  # output to graphml file
  convert_properties(G, str)
  G.save(graph_path + "_mainframe_heuristic.graphml")
#+END_SRC

Plot detail of obtained graph.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/21_plot_heuristic_mainframe.py
  <<imports_&_defaults>>
  <<projection>>
  <<districts>>
  <<graph_utils>>
  <<plot_result>>
  <<min_path_tree>>

  G = load_graph(graph_path + "_mainframe_heuristic.graphml")
  convert_properties(G, float)

  fig = plt.figure(figsize=(6, 6), frameon=False)
  ax = fig.gca()

  plot_result(G,
              ax=ax,
              projection=projection,
              edges_linewidth=0.5,
              root_markersize=15, cut=False)

  aachen_border = cascaded_union(district_map.geometry)
  # remove hole in union
  aachen_border = aachen_border.buffer(100).buffer(-100)

  gpd.GeoDataFrame({'geometry': [aachen_border]}).plot(color='white',
                                                       edgecolor='black',
                                                       linewidth=0.5,
                                                       ax=ax,
                                                       zorder=-2)

  plt.tight_layout(rect=[-0.2, -0.1, 1.2, 1.05])

  # plt.show()
  plt.savefig('figures/heuristic_mainframe.png', dpi=250)
  plt.close('all')
#+END_SRC

* Output abstract topology
Write network tree in a `txt` file for making the job for Rust parser easy.

#+BEGIN_SRC python :noweb yes :var graph_path=flatten(graph_path) :tangle scripts/aachen_net/22_output_topology.py
  <<imports_&_defaults>>
  <<projection>>
  <<graph_utils>>

  G_complete = load_graph(graph_path + "_complete.graphml")
  G_dslam = load_graph(graph_path + "_DSLAM_heuristic.graphml")
  G_2router = load_graph(graph_path + "_2router_heuristic.graphml")
  G_mainframe = load_graph(graph_path + "_mainframe_heuristic.graphml")

  max_vertex = max(int(v) for v in G_complete.vertices())

  buildings = np.where(G_complete.vp['n_lines'].a > 0)[0]
  building_dslam = G_dslam.vp['father_id'].a[buildings]

  dslams = np.unique(building_dslam)
  dslam_router = G_2router.vp['father_id'].a[dslams]

  routers = np.unique(dslam_router)

  # rename nodes that are both DSLAM and router, etc
  def new_renamer(first_set, second_set, factor):
      mapper = { old_v: old_v + factor
                 for old_v in
                 set(first_set).intersection(set(second_set)) }

      def element_renamer(element):
          if element in mapper:
              return mapper[element]
          else:
              return element

      return element_renamer

  # rename the DSLAMs placed in buildings
  rename_bd = new_renamer(buildings, dslams, factor=max_vertex + 1)

  building_dslam = list(map(rename_bd, building_dslam))
  dslams = list(map(rename_bd, dslams))

  # rename the routers placed in buildings
  rename_br = new_renamer(buildings, routers, factor=2 * max_vertex + 1)

  dslam_router = list(map(rename_br, dslam_router))
  routers = list(map(rename_br, routers))

  # rename the routers placed in the same node of a DSLAM
  rename_dr = new_renamer(dslams, routers, factor=3 * max_vertex + 1)

  dslam_router = list(map(rename_dr, dslam_router))
  routers = list(map(rename_dr, routers))

  assert set(buildings).intersection(set(dslams)) == set()
  assert set(routers).intersection(set(dslams)) == set()
  assert set(buildings).intersection(set(routers)) == set()

  assert set(dslams) == set(building_dslam)
  assert set(routers) == set(dslam_router)

  # assign the mainframe a new ID
  mainframe_id = 10 * max_vertex + 1
  router_mainframe = len(routers) * list([mainframe_id])

  # remap the (now) unique ids to consecutive ones

  # NOTE reserve 0 as mainframe id
  old_ids = [mainframe_id] + list(buildings) + dslams + routers
  new_ids = [x for x in range(len(old_ids))]

  def new_replacer(old_group, new_group):
      mapper = dict(zip(old_group, new_group))

      def _replacer(element):
          return mapper[element]

      return _replacer

  unique_replacer = new_replacer(old_ids, new_ids)
  unique_replacer_reverse = new_replacer(new_ids, old_ids)

  buildings = list(map(unique_replacer, buildings))
  building_dslam = list(map(unique_replacer, building_dslam))
  dslams = list(map(unique_replacer, dslams))
  dslam_router = list(map(unique_replacer, dslam_router))
  routers = list(map(unique_replacer, routers))
  router_mainframe = list(map(unique_replacer, router_mainframe))

  # create an abstract graph and save it to graphml
  g = Graph()

  dslam_vertex = {
      dslam: g.add_vertex()
      for dslam in dslams
  }

  router_vertex = {
      router: g.add_vertex()
      for router in routers
  }

  mainframe_vertex = { 0: g.add_vertex() }

  for dslam, router in zip(dslams, dslam_router):
      g.add_edge(dslam_vertex[dslam],
                 router_vertex[router])

  for router, mainframe in zip(routers, router_mainframe):
      g.add_edge(router_vertex[router],
                 mainframe_vertex[mainframe])

  # add users
  for dslam, vertex in dslam_vertex.items():
      buildings_of_dslam = np.array(building_dslam) == dslam
      buildings_ids = list(map(unique_replacer_reverse,
                               np.array(buildings)[buildings_of_dslam]))

      n_lines = sum(G_complete.vp['n_lines'].a[buildings_ids])

      for _ in range(n_lines):
          user = g.add_vertex()
          g.add_edge(user, vertex)

  # check obtained graph is a tree
  assert len(list(g.vertices())) == len(list(g.edges())) + 1

  components = label_components(g)[1]
  assert np.all(components == components[0])

  # count users: check they are the right amount
  total_n_lines = 0
  for v in g.vertices():
      if g.vertex(v).in_degree():
          total_n_lines += 1

  assert total_n_lines == G_complete.vp['n_lines'].a.sum()



  # mark the root (to pick it for sure)
  g.vp['root'] = g.new_vertex_property("bool")
  g.vp['root'].a = False
  g.vp['root'][mainframe_vertex[0]] =True

  g.save('data/aachen_net/abstract_topology.graphml')
#+END_SRC

#+RESULTS:

# # eval: (add-hook 'after-save-hook 'org-babel-tangle-this-file t t)

* COMMENT Local variables
# Local Variables:
# eval: (add-hook 'org-babel-pre-tangle-hook (lambda () (org-babel-lob-ingest "utils.org")) t t)
# org-confirm-babel-evaluate: nil
# End:
