
* Obtaining the dataset
  Dataset is available in [[http://mawi.wide.ad.jp/mawi/][MAWI Working Group Traffic Archive]].
  Here I chose only few of those, as the work required to process them is huge.

  #+NAME: dataset_urls
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201802061400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806171400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806161400.pcap.gz

  This snippet will download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for ~xargs~ parameters details.
  ~wget -c~ ensure each file is downloaded once, even if it is stopped and resumed.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh
    echo ${dataset_urls[*]} | xargs -n 1 -P 8 wget -q -P pcap_data/ -c
  #+END_SRC

* Preprocessing
** Convert tcpdump files to csv
   Here follows a list of all compressed ~pcap~ files.
   The first one is a testing trace: it will not be analyzed, of course.

   #+NAME: pcap_files
   - pcap_data/dummy_capture.pcap.gz
   - pcap_data/201802061400.pcap.gz
   - pcap_data/201806171400.pcap.gz
   - pcap_data/201806161400.pcap.gz

   We are interested only in some properties of the traffic, whose Wireshark keywords are further listed.
   Note that time is expressed in seconds and packet length in bytes.

   #+NAME: field_names
   - frame.time_epoch
   - ip.src
   - ip.dst
   - frame.len
   - frame.protocols

   Now we are ready to convert all ~pcap~ files to ~csv~.

   #+BEGIN_SRC bash :var field_names=field_names pcap_files=pcap_files :results output :tangle scripts/2_convert.sh
     # put "-e " before each field
     field_params=" -e $(echo ${field_names[*]} | sed "s/ / -e /g")"

     for pcap_file in "${pcap_files[@]}"
     do
         output_path="${pcap_file%%.*}.csv.gz"

         echo ""
         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             echo "CONVERT: Jumping $pcap_file, output already exists"
             # continue
         else
             echo "CONVERT: Working on $pcap_file"
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         echo "  Creating temporary directory"
         mkdir -p pcap_data/temp/

         echo "  Splitting main file"
         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         pv $pcap_file | gunzip -c | tcpdump -r - -w $slices_path -C 100

         echo "  Working on each slice"
         ls $slices_path* \
             | sed "s/ /\n/g" \
             | parallel --bar -P 8 \
                        tshark -r $slice \
                        -T fields ${field_params} \
                        -Y "ip.src" \
                        -E separator=, -E occurrence=f > "$slice.dump"

         echo "  Merging results"
         # write first row of csv output and then everything else
         title=$(echo ${field_names[*]} | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

** Normalize time series
   To improve the data structure, every timestamp is set to be relative to the first aquired data.
   Lines are also sorted to ensure time order is correct.

   #+BEGIN_SRC bash :tangle scripts/3_normalize_time.sh :var pcap_files=pcap_files
     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}.csv.gz"

         echo ""
         if [ ! -f $csv_file ]; then
             echo "NORMALIZE: no csv file available"
             continue
         else
             echo "NORMALIZE: Working on $csv_file"
         fi

         # make time start from 0
         awk_snippet='$1 < min_time && NR > 1 { min_time=$1 }
                      END                     { print min_time }'
         min_time=$(zcat $csv_file | awk -v min_time=inf -F, "$awk_snippet")

         awk_snippet='BEGIN   { FS=","; OFS=","}
                           NR == 1 { print $0 }
                           NR > 1  { $1 = $1 - min_time; print $0 }'
         pv $csv_file \
             | gunzip -c \
             | awk -v min_time=$min_time -v CONVFMT=%.12f "$awk_snippet" \
             | gzip -c > ${csv_file}_temp

         # sort by time, to be 100% sure the time serie is in order
         (
             zcat ${csv_file}_temp | head -n1; # ignore the title
             zcat ${csv_file}_temp | tail -n +2 | LC_ALL=C sort -k1 -n -t,
         ) \
             | gzip -c > ${csv_file}

         rm ${csv_file}_temp
     done
   #+END_SRC

* Analysis
** Interarrival distribution
   Time deltas are stored in a proper ~csv~ in order to measure interarrival distribution.

   #+BEGIN_SRC bash :tangle scripts/4_interarrival.sh :var pcap_files=pcap_files :results none
     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}.csv.gz"

         echo ""
         if [ ! -f $csv_file ]; then
             echo "INTERARRIVAL: no csv file available"
             continue
         else
             echo "INTERARRIVAL: Working on $csv_file"
         fi

         awk_snippet='BEGIN   { FS=","; OFS="," }
                      NR == 1 {
                         $1="interarrival_times";
                         print $0
                      }
                      NR == 2 { prev_time = $1 }
                      NR > 2  {
                        delta = $1 - prev_time;
                        prev_time = $1;
                        $1 = delta;
                        print $0;
                      }'

         pv $csv_file \
             | gunzip -c \
             | awk -v CONVFMT=%.12f "$awk_snippet" \
             | gzip -c > ${csv_file%%.*}_interarrival.csv.gz
     done
   #+END_SRC

   I decided to perform the binning in Rust, since it is faster than Python.
   Org babel is not well supported for Rust though, so I tangle the project from here in ~script/~ directory.

   #+BEGIN_SRC toml :tangle scripts/interarrival_binner/Cargo.toml
     [package]
     name = "interarrival_binner"
     version = "0.1.0"
     authors = ["Enrico Lovisotto <enricolovisotto@gmail.com>"]

     [dependencies]
     csv = "1"
     itertools-num = "0.1"
   #+END_SRC

   Script reads from ~stdin~ and outputs to ~stdout~, for ~bash~ integration.

   #+BEGIN_SRC rust :tangle scripts/interarrival_binner/src/main.rs
     extern crate csv;
     extern crate itertools_num;

     use std::io;
     use std::io::prelude::*;
     use std::env;
     use std::process::exit;

     use itertools_num::linspace;
     fn main() {
         let args: Vec<String> = env::args().collect();

         if args.len() != 4 {
             println!("Invalid number of arguments ({}), exiting", args.len());
             exit(1);
         }

         // parse minimum and maximum
         let min = match args[1].parse::<f32>() {
             Ok(number) => number,
             Err(_) => {
                 println!("Invalid maximum");
                 exit(1);
             }
         };

         let max = match args[2].parse::<f32>() {
             Ok(number) => number,
             Err(_) => {
                 println!("Invalid maximum");
                 exit(1);
             }
         };

         let n_bins = match args[3].parse::<usize>() {
             Ok(number) => number,
             Err(_) => {
                 println!("Invalid n_bins");
                 exit(1);
             }
         };

         if max < min {
             println!("Maximum < minumum");
             exit(1);
         }

         // create bins
         let bins_limits: Vec<f32> = linspace::<f32>(min, max, n_bins).collect();
         let mut counts: Vec<i64> = vec![0; bins_limits.len() - 1];

         let stdin = io::stdin();
         let stdin = stdin.lock();

         let mut rdr = csv::Reader::from_reader(stdin);
         for result in rdr.records() {
             let record = result.unwrap();
             let value = &record[0].parse::<f32>().unwrap();

             // assign to proper bin
             let mut assigned = false;
             for (index, upper_limit) in bins_limits.iter().enumerate() {
                 // ignore global minimum in bins limits
                 if index == 0 {
                     continue;
                 }

                 if value <= upper_limit {
                     counts[index - 1] += 1;
                     assigned = true;
                     break;
                 }
             }

             // check everything is ok
             if !assigned {
                 println!("error with {}", value);
                 exit(1);
             }
         }
         println!("{:?}\n{:?}", bins_limits, counts);
     }
   #+END_SRC

   Actually apply the Rust binner, after having gathered maximum and minimum for the interarrival realizations.

   #+NAME: n_bins
   7000

   #+BEGIN_SRC bash :tangle scripts/5_distribution.sh :var pcap_files=pcap_files n_bins=n_bins :results none
     if [ ! -f pcap_data/min_max_interarrival.txt ]; then
         echo "  Finding global minimum and maximum interarrival time across captures"

         # ensure field is numeric, not an header
         awk_snippet='BEGIN { FS="," }
                      ($1 == $1+0) && $1 > max { max=$1 }
                      ($1 == $1+0) && $1 < min { min=$1 }
                      END { print min " " max }'

         read min max <<< $(echo ${pcap_files[*]} \
                                | sed "s/\.pcap/_interarrival.csv/g" \
                                | xargs pv \
                                | gunzip -c \
                                | awk -v min=inf -v max=0 -v CONVFMT=%.12f "$awk_snippet")

         # cache the two numbers
         echo "$min $max" > pcap_data/min_max_interarrival.txt
     else
         echo "  Reading cached global minimum and maximum interarrival time across captures"

         read min max < pcap_data/min_max_interarrival.txt
     fi

     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}_interarrival.csv.gz"

         echo "  Binning $csv_file"
         pv $csv_file \
             | gunzip -c \
             | cargo run --release --manifest-path=scripts/interarrival_binner/Cargo.toml $min $max $n_bins 2> /dev/null \
                     > ${pcap_file%%.*}.distribution
     done
   #+END_SRC

   Finally plot the results.

   #+BEGIN_SRC python :tangle scripts/6_plotting.py :var pcap_files=pcap_files n_bins=n_bins
     import numpy as np
     import matplotlib.pyplot as plt

     for element in pcap_files:
         pcap_file = element[0]

         distribution_path = pcap_file.replace('.pcap.gz', '.distribution')

         with open(distribution_path, 'r') as distribution_file:
             line = distribution_file.next()
             bins = [float(element) for element in line[1:-2].split(', ')]

             line = distribution_file.next()
             counts = [int(element) for element in line[1:-2].split(', ')]

             plt.hist(bins[:-1], weights=counts, bins=bins, log=True)
             plt.show()
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
