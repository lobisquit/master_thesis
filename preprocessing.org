
* Obtaining the dataset
  Dataset is available in [[http://mawi.wide.ad.jp/mawi/][MAWI Working Group Traffic Archive]].
  Here I chose only few of those, as the work required to process them is huge.

  #+NAME: dataset_urls
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201802061400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806171400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806161400.pcap.gz

  Download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for details.
  ~wget -c~ ensure each file is not downloaded twice.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh
    echo ${dataset_urls[*]} | xargs -n 1 -P 8 wget -q -P pcap_data -c
  #+END_SRC

* Preprocessing
** Convert tcpdump files to csv
   Here follows WireShark names for fields we are interested into.

   #+NAME: field_names
   - frame.time_epoch
   - ip.src
   - ip.dst
   - frame.len
   - frame.protocols

   Here follows a list of all compressed ~pcap~ files.

   #+NAME: pcap_files
   - pcap_data/dummy_capture.pcap.gz

   Perform actual procedure, using previous information.

   #+BEGIN_SRC bash :var field_names=field_names pcap_files=pcap_files :results output :tangle scripts/2_convert.sh
     # put "-e " before each field
     field_params=" -e $(echo ${field_names[*]} | sed "s/ / -e /g")"

     for pcap_file in "${pcap_files[@]}"
     do
         output_path="${pcap_file%%.*}.csv.gz"

         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             echo "Jumping $pcap_file, output already exists"
             continue
         else
             echo "Working on $pcap_file"
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         echo "  Creating temporary directory"
         mkdir -p pcap_data/temp/

         echo "  Splitting main file"
         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         pv $pcap_file | gunzip -c | tcpdump -r - -w $slices_path -C 100

         echo "  Working on each slice"
         for slice in $(ls $slices_path* | sed "s/ /\n/g")
         do
             tshark -r $slice \
                    -T fields ${field_params} \
                    -Y "ip.src" \
                    -E separator=, -E occurrence=f > "$slice.dump"
         done

         echo "  Merging results"
         # write first row of csv output and then everything else
         title=$(echo ${field_names[*]} | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

** Normalize time series
   Every file time stamp are set relative to the first aquired data.
   Lines are also sorted to ensure order is correct.

   #+BEGIN_SRC bash :tangle scripts/3_normalize_time.sh :var pcap_files=pcap_files
     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}.csv.gz"

         # make time start from 0
         min_time=$(zcat $csv_file \
                        | awk -v min_time=inf -F, '$1 < min_time && NR>1 { min_time=$1 }
                                                   END {print min_time}')

         echo "Working on $csv_file"
         pv $csv_file \
             | zcat \
             | awk -v min_time=$min_time 'BEGIN {FS=","; OFS=","}
                                          NR == 1 {print $0}
                                          NR > 1 { $1=$1-min_time; print $0 }' \
                                              | gzip -c > ${csv_file}_temp

         # sort by time, to be 100% sure the time serie is in order
         (
             zcat ${csv_file}_temp | head -n1; # ignore the title
             zcat ${csv_file}_temp | tail -n +2 | LC_ALL=C sort -k1 -n -t,
         ) \
             | gzip -c > ${csv_file}

         rm ${csv_file}_temp
     done
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
