
* Obtaining the dataset
  Dataset is available in [[http://mawi.wide.ad.jp/mawi/][MAWI Working Group Traffic Archive]].
  Here I chose only few of those, as the work required to process them is huge.

  #+NAME: dataset_urls
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201803151400.pcap.gz

  #+BEGIN_SRC sh :var dataset_urls=dataset_urls :tangle scripts/1_download.sh
    for url in $(echo $dataset_urls | sed "s/ /\n/g")
    do
        wget $url -P pcap_files
    done
  #+END_SRC

* Preprocessing
** Convert tcpdump files to csv
   Here follows WireShark names for fields we are interested into.
   #+NAME: field_names
   - frame.time_epoch
   - ip.src
   - ip.dst
   - frame.len

   Here follows a list of all compressed ~pcap~ files.
   #+NAME: pcap_files
   - pcap_data/201805090130.pcap.gz

   Perform actual procedure, using previous information.
   #+BEGIN_SRC sh :var field_names=field_names pcap_files=pcap_files :results none :tangle scripts/2_convert.sh
     # put "-e " before each field
     field_params=" -e $(echo $field_names | sed "s/ / -e /g")"

     for pcap_file in $(echo $pcap_files | sed "s/ /\n/g")
     do
         output_path="${pcap_file%%.*}.csv.gz"

         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             echo "Jumping $pcap_file, output already exists"
             continue
         else
             echo "Working on $pcap_file"
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         mkdir -p pcap_data/temp/

         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         gunzip -c $pcap_file | tcpdump -r - -w $slices_path -C 100

         for slice in $(ls $slices_path* | sed "s/ /\n/g")
         do
             tshark -r $slice \
                    -T fields ${field_params} \
                    -Y "ip.src" \
                    -E separator=, -E occurrence=f > "$slice.dump"
         done

         # write first row of csv output and then everything else
         title=$(echo $field_names | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

   Convert ~csv~ dataset to ~HDF5~.
   #+BEGIN_SRC python :var pcap_files=pcap_files :results output :tangle scripts/second_step.py
     import pandas as pd

     stems = [pcap_path[0].split('.')[0] for pcap_path in pcap_files]
     csv_files = ['{}.csv.gz'.format(stem) for stem in stems]
     h5_files = ['{}.csv.'.format(stem) for stem in stems]

     for stem in stems:
         csv_file = '{}.csv.gz'.format(stem)
         print('Working on {}'.format(csv_file))
         # chunksize is the number of rows
         chunksize = 1000
         current_data = pd.read_csv(csv_file, chunksize=chunksize)

         with pd.HDFStore('{}.h5'.format(stem), mode='w', complevel=9) as store:
             for n, chunk in enumerate(current_data):
                 print("{} rows parsed".format(n * chunksize), end='\r')
                 store.append('table', chunk, index=False)
   #+END_SRC

   #+RESULTS:

* Useful python code
  Convert IP addresses to integer and viceversa.

  #+BEGIN_SRC python :results output
    import ipaddress

    print(int(ipaddress.IPv4Address("192.168.0.1")))
    print(str(ipaddress.IPv4Address(3232235521)))
  #+END_SRC

  #+RESULTS:
  : 3232235521
  : 192.168.0.1

* Local variables
  # Local Variables:
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # End:
