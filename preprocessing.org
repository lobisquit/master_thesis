
* Obtaining the dataset
  Dataset is available in [[http://mawi.wide.ad.jp/mawi/][MAWI Working Group Traffic Archive]].
  Here I chose only few of those, as the work required to process them is huge.

  #+NAME: dataset_urls
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201802061400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806171400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806161400.pcap.gz

  This snippet will download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for ~xargs~ parameters details.
  ~wget -c~ ensure each file is downloaded once, even if it is stopped and resumed.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh
    echo ${dataset_urls[*]} | xargs -n 1 -P 8 wget -q -P pcap_data/ -c
  #+END_SRC

* Preprocessing
** Convert tcpdump files to csv
   Here follows a list of all compressed ~pcap~ files.
   The first one is a testing trace: it will not be analyzed, of course.

   #+NAME: pcap_files
   - pcap_data/dummy_capture.pcap.gz
   - pcap_data/201802061400.pcap.gz
   - pcap_data/201806171400.pcap.gz
   - pcap_data/201806161400.pcap.gz

   We are interested only in some properties of the traffic, whose Wireshark keywords are further listed.
   Note that time is expressed in seconds and packet length in bytes.

   #+NAME: field_names
   - frame.time_epoch
   - ip.src
   - ip.dst
   - frame.len
   - frame.protocols

   Now we are ready to convert all ~pcap~ files to ~csv~.

   #+BEGIN_SRC bash :var field_names=field_names pcap_files=pcap_files :results output :tangle scripts/2_convert.sh
     # put "-e " before each field
     field_params=" -e $(echo ${field_names[*]} | sed "s/ / -e /g")"

     for pcap_file in "${pcap_files[@]}"
     do
         output_path="${pcap_file%%.*}.csv.gz"

         echo ""
         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             echo "CONVERT: Jumping $pcap_file, output already exists"
             # continue
         else
             echo "CONVERT: Working on $pcap_file"
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         echo "  Creating temporary directory"
         mkdir -p pcap_data/temp/

         echo "  Splitting main file"
         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         pv $pcap_file | gunzip -c | tcpdump -r - -w $slices_path -C 100

         echo "  Working on each slice"

         slices=$(ls $slices_path* | sed "s/ /\n/g")
         n_slices=$(echo $slices | wc -l)
         for slice in $slices
         do
             tshark -r $slice \
                    -T fields ${field_params} \
                    -Y "ip.src" \
                    -E separator=, -E occurrence=f > "$slice.dump"

             printf "  $(ls pcap_data/temp/*.dump 2> /dev/null | wc -l)/$n_slices\r"
         done

         echo "  Merging results"
         # write first row of csv output and then everything else
         title=$(echo ${field_names[*]} | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

** Normalize time series
   To improve the data structure, every timestamp is set to be relative to the first aquired data.
   Lines are also sorted to ensure time order is correct.

   #+BEGIN_SRC bash :tangle scripts/3_normalize_time.sh :var pcap_files=pcap_files
     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}.csv.gz"

         echo ""
         if [ ! -f $csv_file ]; then
             echo "NORMALIZE: no csv file available"
             continue
         else
             echo "NORMALIZE: Working on $csv_file"
         fi

         # make time start from 0
         awk_snippet='$1 < min_time && NR>1 { min_time=$1 }
                      END                   { print min_time }'
         min_time=$(zcat $csv_file | awk -v min_time=inf -F, "$awk_snippet")

         awk_snippet='BEGIN   { FS=","; OFS=","}
                           NR == 1 { print $0 }
                           NR > 1  { $1 = $1 - min_time; print $0 }'
         pv $csv_file \
             | gunzip -c \
             | awk -v min_time=$min_time -v CONVFMT=%.12f "$awk_snippet" \
             | gzip -c > ${csv_file}_temp

         # sort by time, to be 100% sure the time serie is in order
         (
             zcat ${csv_file}_temp | head -n1; # ignore the title
             zcat ${csv_file}_temp | tail -n +2 | LC_ALL=C sort -k1 -n -t,
         ) \
             | gzip -c > ${csv_file}

         rm ${csv_file}_temp
     done
   #+END_SRC

** Interarrival distribution
   Time deltas are stored in a proper ~csv~ in order to measure interarrival distribution.

   #+BEGIN_SRC bash :tangle scripts/4_interarrival.sh :var pcap_files=pcap_files :results none
     for pcap_file in "${pcap_files[@]}"
     do
         csv_file="${pcap_file%%.*}.csv.gz"

         echo ""
         if [ ! -f $csv_file ]; then
             echo "INTERARRIVAL: no csv file available"
             continue
         else
             echo "INTERARRIVAL: Working on $csv_file"
         fi

         awk_snippet='BEGIN   { FS=","; OFS="," }
                      NR == 1 { print "interarrival_times" }
                      NR == 2 { prev_time = $1 }
                      NR > 2  {
                        delta = $1 - prev_time;
                        prev_time = $1;
                        $1 = delta;
                        print $0;
                        # printf("%.12f\n", delta);
                      }'

         pv $csv_file \
             | gunzip -c \
             | awk -v CONVFMT=%.12f "$awk_snippet" \
             | gzip -c > ${csv_file%%.*}_interarrival.csv.gz
     done
   #+END_SRC

* Local variables
  # Local Variables:
  # sh-indent-after-continuation: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
