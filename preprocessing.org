
* Obtaining the dataset
  Dataset is available in [[http://mawi.wide.ad.jp/mawi/][MAWI Working Group Traffic Archive]].
  Here I chose only few of those, as the work required to process them is huge.

  #+NAME: dataset_urls
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201802061400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806171400.pcap.gz
  - http://mawi.nezu.wide.ad.jp/mawi/samplepoint-F/2018/201806161400.pcap.gz

  Download all of them in a parallel fashion, see [[https://stackoverflow.com/a/11850469][here]] for details.
  ~wget -c~ ensure each file is not downloaded twice.

  #+BEGIN_SRC bash :var dataset_urls=dataset_urls :tangle scripts/1_download.sh
    echo ${dataset_urls[*]} | xargs -n 1 -P 8 wget -q -P pcap_data -c
  #+END_SRC

* Preprocessing
** Convert tcpdump files to csv
   Here follows WireShark names for fields we are interested into.

   #+NAME: field_names
   - frame.time_epoch

   - ip.src
   - ip.dst
   - frame.len
   - frame.protocols

   Here follows a list of all compressed ~pcap~ files.

   #+NAME: pcap_files
   - pcap_data/dummy_capture.pcap.gz

   Perform actual procedure, using previous information.

   #+BEGIN_SRC bash :var field_names=field_names pcap_files=pcap_files :results output :tangle scripts/2_convert.sh
     # put "-e " before each field
     field_params=" -e $(echo ${field_names[*]} | sed "s/ / -e /g")"

     for pcap_file in "${pcap_files[@]}"
     do
         output_path="${pcap_file%%.*}.csv.gz"

         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             echo "Jumping $pcap_file, output already exists"
             continue
         else
             echo "Working on $pcap_file"
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         echo "  Creating temporary directory"
         mkdir -p pcap_data/temp/

         echo "  Splitting main file"
         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         pv $pcap_file | gunzip -c | tcpdump -r - -w $slices_path -C 100

         echo "  Working on each slice"
         for slice in $(ls $slices_path* | sed "s/ /\n/g")
         do
             tshark -r $slice \
                    -T fields ${field_params} \
                    -Y "ip.src" \
                    -E separator=, -E occurrence=f > "$slice.dump"
         done

         echo "  Merging results"
         # write first row of csv output and then everything else
         title=$(echo ${field_names[*]} | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

* Convert data to time series
  #+BEGIN_SRC bash :tangle scripts/3_normalize_time.sh :var pcap_files=pcap_files
    for pcap_file in "${pcap_files[@]}"
    do
        csv_file="${pcap_file%%.*}.csv.gz"

        # make time start from 0
        min_time=$(zcat $csv_file \
                       | awk -v min_time=inf -F, '$1<min_time && NR>1 { min_time=$1 } END {print min_time}')

        echo "Working on $csv_file"
        pv $csv_file \
            | zcat \
            | awk -v min_time=$min_time 'BEGIN {FS=","; OFS=","} NR==1 {print $0} NR>1 { $1=$1-min_time; print $0 }' \
            | gzip -c > ${csv_file}_temp

        mv ${csv_file}_temp $csv_file
    done
  #+END_SRC

* Useful python code
  Convert IP addresses to integer and viceversa.

  #+BEGIN_SRC python :results output
    import ipaddress

    print(int(ipaddress.IPv4Address("192.168.0.1")))
    print(str(ipaddress.IPv4Address(3232235521)))
  #+END_SRC

  #+RESULTS:
  : 3232235521
  : 192.168.0.1



* Local variables

  # Local Variables:
  # sh-indent-after-continuation: nil
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # eval: (add-hook 'after-save-hook 'org-babel-tangle t t)
  # End:
