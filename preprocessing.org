* Preprocessing
** Convert tcpdump files to csv
   Here follows WireShark names for fields we are interested into.
   #+NAME: field_names
   - frame.time_epoch
   - ip.src
   - ip.dst
   - frame.len

   Here follows a list of all compressed ~pcap~ files.
   #+NAME: pcap_files
   - pcap_data/201805090130.pcap.gz

   Perform actual procedure, using previous information.
   #+BEGIN_SRC sh :var field_names=field_names pcap_files=pcap_files :results none
     # put "-e " before each field
     field_params=" -e $(echo $field_names | sed "s/ / -e /g")"

     for pcap_file in $(echo $pcap_files | sed "s/ /\n/g")
     do
         output_path="${pcap_file%%.*}.csv.gz"

         # stop iteration immediately if output file already exists
         if [ -f $output_path ]; then
             continue
         fi

         # PROBLEM tshark has a memory leak and cannot deal with file that are too big
         # we need to split each source file in 100MB chunks using tcpdump
         mkdir -p pcap_data/temp/

         slices_path="pcap_data/temp/slice_$(basename ${pcap_file%%.*}).pcap"
         gunzip -c $pcap_file | tcpdump -r - -w $slices_path -C 100

         for slice in $(ls $slices_path* | sed "s/ /\n/g")
         do
             tshark -r $slice \
                    -T fields ${field_params} \
                    -Y "ip.src" \
                    -E separator=, -E occurrence=f > "$slice.dump"
         done

         # write first row of csv output and then everything else
         title=$(echo $field_names | sed "s/ /,/g")
         (echo $title && cat $slices_path*.dump) | gzip > $output_path

         rm -rf pcap_data/temp/
     done
   #+END_SRC

* Local variables
  # Local Variables:
  # eval: (add-hook 'before-save-hook (lambda () (indent-region (point-min) (point-max) nil)) t t)
  # End:
